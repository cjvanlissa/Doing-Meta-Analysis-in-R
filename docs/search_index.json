[
["index.html", "Doing Meta-Analysis in R and exploring heterogeneity using metaforest A Hands-on Guide Chapter 1 About this Guide", " Doing Meta-Analysis in R and exploring heterogeneity using metaforest A Hands-on Guide Caspar van Lissa¹ ¹Utrecht University Chapter 1 About this Guide This guide is based on the book ‘Doing Meta-Analysis in R’, by Mathias Harrer, Pim Cuijpers, &amp; David Ebert, and was adapted to focus on the metafor package, and exploring heterogeneity using metaforest. The original can be found here: https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/ The guide will show you how to: Get R and RStudio set for your Meta-Analysis Get your data into R Prepare your data for the meta-analysis Perform fixed-effect and random-effects meta-analysis using the meta and metaforpackages Analyse the heterogeneity of your results Tackle heterogeneity using subgroup analyses and meta-regression Check if selective outcome reporting (publication bias) is a present in your data Control for selective outcome reporting and publication bias What this guide will not cover Although this guide will provide some information on the statistics behind meta-analysis, it will not give you an in-depth introduction into how meta-analyses are calculated statistically. It is also beyond the scope of this guide to advise in detail which meta-analytical strategy is suited best in which contexts, and on how the search, study inclusion and reporting of meta-analyses should be conducted. The Cochrane Handbook for Systematic Reviews of Interventions, however, should be a great source to find more information on these topics. Generally, there a two other sources to recommended when conducting Meta-Analyses: If you’re looking for a easily digestable, hands-on introduction on how Meta-Analyses are conducted, we can recommend Pim Cuijpers’ online courses on Meta-Analysis. The courses are freely available on YouTube. To have a look, click here. If you’re interested in more details on how to conduct Meta-Analyses in R, you can either have a look at Wolfgang Viechtbauer’s page for the metafor package (Link). Or you can consult a book on the meta package which was recently published (Schwarzer, Carpenter, and Rücker 2015). How to get the R code for this guide All code behind this book is available online on GitHub. We have created a website containing a download link for all codes, and a quick guide on how to get the code running on your computer. The site can be found here. How to cite this guide Harrer, M., Cuijpers, P. &amp; Ebert, D. D. (2019). Doing Meta-Analysis in R: A Hand-on Guide. https://bookdown.org/MathiasHarrer/Doing_Meta_Analysis_in_R/. To get started, proceed to the next chapter! References "],
["preparing-for-the-course.html", "Chapter 2 Preparing for the course", " Chapter 2 Preparing for the course Before we start the course, we have to download and prepare a computer program which allows us to use R for programming. Probably the best option for this at the moment is RStudio. This program gives us a user interface which makes it easier to overlook our data, packages and output. The best part is that RStudio is completely free and can be downloaded anytime. In this Chapter, we’ll focus on how you can install RStudio on your computer. We’ll also provide some general information on R, and how you can get help if you get error messages. If you already have RStudio installed on your computer, and if you’re an experienced R user already, all of this might be nothing new for you. You may skip this chapter then. Especially if you have never used R before, we would like to consider this Chapter essential, as it gives you some input on how R works, and how we can use it for our data analyses. "],
["RStudio.html", "2.1 Getting RStudio to run on your computer", " 2.1 Getting RStudio to run on your computer As a prerequisite for this guide, you need to have RStudio and a few essential R packages installed. You have to follow these steps to get your RStudio set. Download RStudio on the RStudio Website (Link). It’s free! If you do not have R installed yet, you will have to install the latest R Version before you can use RStudio. You can get the latest R version here. Once RStudio is running, open the Console on the bottom left corner of your screen. We will now install a few packages using R Code. Here’s an overview of the packages, and why we need them: Package Description tidyverse This is a large package containing various functions to tidy up your data meta This package is a so-called wrapper for many meta-analytic functions and makes it easy to code different meta-analyses metaforest This package is used to explore heterogeneity using the random forests algorithm. It also installs the R-package ‘metafor’, which is one of the most commonly used meta-analysis packages. 5. To install these packages, we use the install.packages() function in R. One package after another, our code should look like this: install.packages(&quot;tidyverse&quot;) install.packages(&quot;meta&quot;) # To install the &#39;metaforest&#39; package, we use the development version, # which contains some functions for this course. # First, install the &#39;devtools&#39; package, which allows you to install packages # directly from github install.packages(&quot;devtools&quot;) # Load the devtools package library(devtools) # Install metaforest from github: install_github(&quot;cjvanlissa/metaforest&quot;) Don’t forget to put the packages in &quot;&quot;. Otherwise, you will get an error message. "],
["starting-a-new-project-in-rstudio.html", "2.2 Starting a new project in Rstudio", " 2.2 Starting a new project in Rstudio To keep all your work organized, you should use a project. In Rstudio, click File &gt; New Project &gt; New directory &gt; New project. Type the desired directory name in the dialog (give it a meaningful name, e.g. “My Meta-Analysis”), and use ‘Browse’ if you need to change the directory where you store your projects. Now, in your project, click File &gt; New file &gt; R script. This script file works just like notepad, or the syntax editor in SPSS: You type plain text, but you can run it any time you want. Conduct all of the exercises in this script file. "],
["additional-resources-optional.html", "2.3 Additional resources (optional)", " 2.3 Additional resources (optional) In order to get the most out of this guide, it’s helpful (but not essential) if you have some programming experience already. If you’ve never programmed before, you might find Hands-On Programming with R (Grolemund 2014) to be a useful primer. There are three things you need to run the code: R, RStudio, and collection of R packages. Packages are the fundamental units of reproducible R code. They include reusable functions, the documentation that describes how to use them, and sample data. Gladly, once you’ve reached this point successfully, these three things are set already. Nevertheless, we will have to install and load a few new packages at some place in this guide, for which you can use the install.packages() the same way as you did before. Throughout the guide, a consistent set of conventions is used to refer to code: Functions are in a code font and followed by parentheses, like sum() or mean(). Other R objects (like data or function arguments) are in a code font, without parentheses, like seTE or method.tau. Sometimes, we’ll use the package name followed by two colons, like meta::metagen(). This is also valid R code. This is used so as to not confuse the functions of different packages with the same name. 2.3.1 Getting Help As you start to apply the techniques described in this guide to your data you will soon find questions that the guide does not answer. This section describes a few tips on how to get help. If you get stuck, start with Google. Typically, adding “R” to a search is enough to restrict it to relevant results: if the search isn’t useful, it often means that there aren’t any R-specific results available. Google is particularly useful for error messages. If you get an error message and you have no idea what it means, try googling it. Chances are that someone else has been confused by it in the past, and there will be help somewhere on the web. (If the error message isn’t in English, run Sys.setenv(LANGUAGE = &quot;en&quot;) and re-run the code; you’re more likely to find help for English error messages.) If Google doesn’t help, try stackoverflow. Start by spending a little time searching for an existing answer; including [R] restricts your search to questions and answers that use R. Lastly, if you stumble upon an error (or typos!) in this guide’s text or R syntax, feel free to contact Caspar van Lissa at c.j.vanlissa@uu.nl. References "],
["getting-your-data-into-r.html", "Chapter 3 Getting your data into R", " Chapter 3 Getting your data into R This chapter will tell you about how you can import your effect size data in RStudio. We will also show you a few commands to manipulate data directly in R. "],
["what-should-be-in-the-file.html", "3.1 What should be in the file", " 3.1 What should be in the file To conduct Meta-Analyses in R, you need to have your study data prepared. In primary analysis, you might have a file with rows for people, and columns for variables. In meta-analysis, you will have a file with a row for each effect size. The columns often include the following information: The names of the individual studies, so that they can be easily identified later on. You can use the full reference, or just the first author and publication year of a study (e.g. “Ebert et al., 2018”) The calculated effect size of the study (either Cohen’s d or Hedges’ g, or some other form of effect size The Standard Error (SE) or sampling variance of the calculated effect Often, you do not have the effect size and the variance of the effect size yet. In that case, you need the statistics required to compute it. This could be: The correlation coefficient and number of participants (N) The Mean of both the Intervention and the Control group at the same assessment point The Standard Deviation of both the Intervention and the Control group at the same assessment point The number of participants (N) in each group of the trial Any coded moderators. E.g., if you want to have a look at differences between various study subgroups later on, you also need a subgroup code for each study which signifies to which subgroup it belongs. For example, if a study was conducted in children, you might give it the subgroup code “children”. Continuous moderators can be included in the same way; e.g., the proportion of men in the sample. Working with R, the easiest way is often to store data in EXCEL spreadsheets, but R can read nearly any input format. Google is your friend. One advantage of using an R project is that the project directory is automatically set as the working directory. Just copy your data file to the folder that contains the “.Rproj” file, and you will be able to load files by name. "],
["importing-excel-files.html", "3.2 Importing Excel Files", " 3.2 Importing Excel Files One way to get Excel files directly into R is by using the XLConnect package. For this example, you will need the “happy.xlsx” file. Check if the file is in your project directory. If not, you can download it from https://github.com/cjvanlissa/Doing-Meta-Analysis-in-R/happy.xlsx. Install the package, and try using the readWorksheetFromFile() function to load the data, and assign it to an object called df: # Run this only once, to download and install the package: install.packages(&quot;XLConnect&quot;) # Load the package: library(XLConnect) # Read the &#39;Happy to help&#39; Excel file into &#39;df&#39;: df &lt;- readWorksheetFromFile(&quot;happy.xlsx&quot;, sheet = 1) 3.2.1 Inspect the data R does not work with a single spreadsheet (SPSS or Excel). Instead, it can keep many objects in memory. The object df is a data.frame; an object that behaves similar to a spreadsheet. To see a description of the object, look at the Environment tab in the top right of Rstudio, and click the arrow next to df. As you can see, the on the top-right pane Environment, your file is now listed as a data set in your RStudio environment. You can make a quick copy of this data set by assigning the df object to a new object. This way, you can edit one, and leave the other unchanged. Assign the object df to a new object called happy: happy &lt;- df You can also have a look at the contents of df by clicking the object in the Environment panel, or running the command head(df): Here’s a (shortened) table for the data study_id d vi n1i n2c sex age location donorcode interventioncode controlcode recipients outcomecode Aknin, Barrington-Leigh, et al. (2013) Study 3 0.46 0.0205290 100 100 38 21.0 Canada / South Africa Typical Prosocial Spending Self Help Anonymous Sick Children PN Affect Aknin, Barrington-Leigh, et al. (2013) Study 3 0.13 0.0200422 100 100 38 21.0 Canada / South Africa Typical Prosocial Spending Self Help Anonymous Sick Children Life Satisfaction Aknin, Broesch, et al. (2015) Study 1 0.93 0.1704788 13 13 42 45.0 Vanuatu Typical Prosocial Spending Self Help Family / Friends PN Affect Aknin, Broesch, et al. (2015) Study 2 0.30 0.1011250 20 20 70 3.0 Vanuatu Typical Other Neutral Activity Puppet Other Aknin, Dunn, et al. (2013) Study 3 0.24 0.0805760 25 25 34 21.0 Canada Typical Prosocial Spending Self Help Someone Other Aknin, Fleerackers, et al. (2014) 0.38 0.0342225 60 59 41 19.9 USA Typical Prosocial Spending Self Help Anonymous Sick Children PN Affect "],
["importing-spss-files-optional.html", "3.3 Importing SPSS Files (optional)", " 3.3 Importing SPSS Files (optional) SPSS files can be loaded using the foreign package. For this example, you can download an SPSS file at https://github.com/cjvanlissa/Doing-Meta-Analysis-in-R/problem2.sav. # Install the package, run this only once install.packages(&quot;foreign&quot;) # Load the `foreign` library library(foreign) # Read the SPSS data df_spss &lt;- read.spss(&quot;problem2.sav&quot;, to.data.frame = TRUE, use.value.labels = FALSE) "],
["data-manipulation-optional.html", "3.4 Data manipulation (optional)", " 3.4 Data manipulation (optional) Now that we have the Meta-Analysis data in RStudio, let’s do a few manipulations with the data. These functions might come in handy when were conducting analyses later on. Going back to the output of the str() function, we see that this also gives us details on the type of column data we have stored in our data. There a different abbreviations signifying different types of data. Abbreviation Type Description num Numerical This is all data stored as numbers (e.g. 1.02) chr Character This is all data stored as words log Logical These are variables which are binary, meaning that they signify that a condition is either TRUE or FALSE factor Factor Factors are stored as numbers, with each number signifying a different level of a variable. A possible factor of a variable might be 1 = low, 2 = medium, 3 = high 3.4.1 Converting to factors Let’s look at the different kinds of interventions, df$interventioncode. We can have a look at this variable by typing the name of our dataset, then adding the selector $ and then adding the variable we want to have a look at. This variable is currently a character vector (text). We want it to be a factor: That’s a categorical variable. To convert this to a factor variable now, we use the factor() function. df$interventioncode &lt;- factor(df$interventioncode) df$interventioncode We now see that the variable has been converted to a factor with the levels “Acts of Kindness,”Other“, and”Prosocial Spending&quot;. 3.4.2 Selecting specific studies It may often come in handy to select certain studies for further analyses, or to exclude some studies in further analyses (e.g., if they are outliers). To do this, we can use the filter function in the dplyr package, which is part of the tidyverse package we installed before. So, let’s load the package first. library(dplyr) Let’s say we want to do a Meta-Analysis with studies conducted in the USA, or partly conducted in the USA, only. To do this, we need to create a new dataset containing only these studies using the dplyr::filter() function. The dplyr:: part is necessary as there is more than one `filter function in R, and we want to use to use the one of the dplyrpackage. The R code to store these studies in a new dataset called df_usa looks like this: df_usa &lt;- dplyr::filter(df, location %in% c(&quot;USA&quot;, &quot;USA/Korea&quot;)) Note that the %in%-Command tells the filter function to search for cases whose location is included in the vector c(&quot;USA&quot;, &quot;USA/Korea&quot;). Now, let’s have a look at the new data df_usa we just created. study_id effect_id d vi n1i n2c sex age location donor donorcode interventioniv interventioncode control controlcode recipients outcomedv outcomecode Aknin, Fleerackers, et al. (2014) 6 0.38 0.0342225 60 59 41 19.90 USA Typical Typical Prosocial Purchase Prosocial Spending Personal Purchase Self Help Anonymous Sick Children PANAS PN Affect Aknin, Fleerackers, et al. (2014) 7 0.44 0.0344293 60 59 41 19.90 USA Typical Typical Prosocial Purchase Prosocial Spending Personal Purchase Self Help Anonymous Sick Children ORH Other Donnelly, Grant, et al. (2017) Study 1 21 0.77 0.0373841 59 56 52 22.57 USA Typical Typical Social recycling Other Trash/Recycling Neutral Activity Unknown lab workers H Happiness Donnelly, Grant, et al. (2017) Study 1 22 0.85 0.0369597 59 59 52 22.57 USA Typical Typical Social recycling Other Take item Self Help Unknown lab workers H Happiness Donnelly, Grant, et al. (2017) Study 2b 23 1.25 0.0222388 107 108 50 37.77 USA Typical Typical Social recycling Other Trash Neutral Activity Unknown lab workers PA PN Affect Layous, Kurtz, J, et al. (under review) Study 1 27 0.08 0.0288015 70 69 16 18.55 USA Typical Typical AK Acts of Kindness Track daily activity Neutral Activity Someone known SHS Happiness Note that the function can also be used for any other type of data and variable. We can also use it to e.g., only select studies where the donors were “typical”: df_typical &lt;- dplyr::filter(df, donorcode == &quot;Typical&quot;) 3.4.3 Changing cell values Sometimes, even when preparing your data in EXCEL, you might want to change values in RStudio once you have imported your data. To do this, we have to select a cell in our data frame in RStudio. This can be done by adding [x,y] to our dataset name, where x signifies the number of the row we want to select, and y signifies the number of the column. To see how this works, let’s select a variable using this command first: df[8,1] ## [1] &quot;Aknin, Hamlin, et al. (2012) &quot; We now see the 6th study in our dataframe, and the value of this study for Column 1 (the author name) is displayed. Let’s say we had a typo in this name and want to have it changed. In this case, we have to give this exact cell a new value. df[8,1] &lt;- &quot;Aknin, et al. (2012)&quot; Let’s check if the name has changed. df[8,1] ## [1] &quot;Aknin, et al. (2012)&quot; You can also use this function to change any other type of data, including numericals and logicals. Only for characters, you have to put the values you want to insert in &quot;&quot;. "],
["calc.html", "Chapter 4 Calculating Effect Sizes", " Chapter 4 Calculating Effect Sizes Papers do not always report the effect sizes exactly the way you want to meta-analyze them. This chapter addresses the basics of calculating effect sizes. Meta-Analysis requires an effect size and an estimate of the sampling variance of that effect size for each study. Papers do not always report the effect size, or they report a different effect size than the one you want to use in your meta-analysis. This chapter addresses the basics of calculating effect sizes. It may not immediately be obvious whether a paper reports the necessecary statistics to calculate an effect size. You will need an ‘effect size’ and its sampling variance. As a general guideline, ask yourself these questions: Am I meta-analyzing a descriptive statistic (mean, SD, proportion, Cronbach’s alpha), a measure of association (correlation or bivariate regression coefficient), or a difference (e.g., mean difference)? Is that statistic reported directly? Is its variance or SE reported directly? Do I have the sample size for the total group or for each group I’m comparing? Are measures of variability reported (e.g., SD for each group)? If you cannot figure out whether you have sufficient information to calculate the effect size, I recommend contacting a statistician. Researchers can get quite creative in trying to obtain the relevant information. It is expected that researchers contact authors of papers with incomplete information to request that information. Many journals require the authors to provide this information upon request. Moreover, researchers sometimes use an on-screen ruler (e.g., https://www.arulerforwindows.com/) to measure means and SEs from graphs, if these are not reported in the text of the paper. "],
["fixed.html", "4.1 Calculating standardized mean differences", " 4.1 Calculating standardized mean differences To calculate standardized mean differences (SMD), we need means, SDs, and sample sizes per group. In this example, we’ll be looking at the dat.normand1999 dataset included with metafor: source n1i m1i sd1i n2i m2i sd2i Edinburgh 155 55 47 156 75 64 Orpington-Mild 31 27 7 32 29 4 Orpington-Moderate 75 64 17 71 119 29 Orpington-Severe 18 66 20 18 137 48 Montreal-Home 8 14 8 13 18 11 Montreal-Transfer 57 19 7 52 18 4 Newcastle 34 52 45 33 41 34 Umea 110 21 16 183 31 27 Uppsala 60 30 27 52 23 20 To calculate effect sizes, we use the function metafor::escalc, which incorporates formulas to compute many different effect sizes. A detailed explanation, with references to the formulas used, can be found by selecting the function, and pressing F1. df_smd &lt;- escalc(measure = &quot;SMD&quot;, m1i = m1i, m2i = m2i, sd1i = sd1i, sd2i = sd2i, n1i = n1i, n2i = n2i, data = dat.normand1999) df_smd source n1i m1i sd1i n2i m2i sd2i yi vi Edinburgh 155 55 47 156 75 64 -0.3551696 0.0130647 Orpington-Mild 31 27 7 32 29 4 -0.3479400 0.0644689 Orpington-Moderate 75 64 17 71 119 29 -2.3175692 0.0458121 Orpington-Severe 18 66 20 18 137 48 -1.8879823 0.1606177 Montreal-Home 8 14 8 13 18 11 -0.3839641 0.2054333 Montreal-Transfer 57 19 7 52 18 4 0.1721487 0.0369106 Newcastle 34 52 45 33 41 34 0.2720521 0.0602671 Umea 110 21 16 183 31 27 -0.4245963 0.0148630 Uppsala 60 30 27 52 23 20 0.2895562 0.0362717 Note that the function returns the original data, with two added columns: yi and vi. These are the SMD effect size and its variance. "],
["formula.html", "4.2 From formulas", " 4.2 From formulas Sometimes, you want to use a specific formula to calculate the effect size - or you just want the formula to be explicit, not hidden away inside escalc. You can transcribe the formula into R syntax, and compute the effect size manually. For this example, we use the the dat.molloy2014 dataset, included with metafor, which contains correlations, and their sample sizes: authors ni ri Axelsson et al. 109 0.187 Axelsson et al. 749 0.162 Bruce et al. 55 0.340 Christensen et al. 107 0.320 Christensen &amp; Smith 72 0.270 Cohen et al. 65 0.000 Dobbels et al. 174 0.175 Ediger et al. 326 0.050 Insel et al. 58 0.260 Jerant et al. 771 0.010 Moran et al. 56 -0.090 O’Cleirigh et al. 91 0.370 Penedo et al. 116 0.000 Quine et al. 537 0.150 Stilley et al. 158 0.240 Wiebe &amp; Christensen 65 0.040 Because correlations are bounded by [-1, 1], they are often Fisher-transformed prior to meta-analysis. The formula for Fisher’s r-to-z transformation is: \\[ z = .5 * ln(\\frac{1+r}{1-r}) \\] In R-syntax, that formula is expressed as: z &lt;- .5 * log((1+r)/(1-r)). We can calculate a new column in the data, using this formula, by substituting r with the column in the data that contains the correlations: df_cor &lt;- dat.molloy2014 # Compute new column: df_cor$zi &lt;- .5 * log((1+df_cor$ri)/(1-df_cor$ri)) Alternatively, we can store the calculation as a function, which makes the code a bit cleaner: # Create function r_to_z, which takes r as input: r_to_z &lt;- function(r){.5 * log((1+r)/(1-r))} # Apply the function to compute new column: df_cor$zi &lt;- r_to_z(df_cor$ri) The sampling variance for the transformed correlations is: \\[ V_z = \\frac{1}{n-3} \\] So by the same process, we can add the sampling variance to the data: # Create function v_z, which takes n as input: v_z &lt;- function(n){1/(n-3)} # Apply the function to compute new column: df_cor$vi &lt;- v_z(df_cor$ni) "],
["pool.html", "Chapter 5 Pooling Effect Sizes", " Chapter 5 Pooling Effect Sizes Now, let’s get to the core of every Meta-Analysis: pooling your effect sizes to get one overall effect size estimate of the studies. When pooling effect sizes in Meta-Analysis, there are two basic approaches: the Fixed-Effect-Model, or the Random-Effects-Model (Borenstein et al. 2011). The fixed effect model assumes that one true effect size exists in the population; the random effects model assumes that the true effect varies (and is normally distributed). There is an extensive debate on which model fits best in which context (Fleiss 1993), with no clear consensus in sight. Although it has been recommended to only resort to the Random-Effects-Pooling model in clinical psychology and the health sciences (Cuijpers 2016), we will describe how to conduct both in R here. Both of these models only require an effect size, and a dispersion (variance) estimate for each study, of which the inverse is taken. This is why the methods are often called generic inverse-variance methods. We will describe in-depth how to conduct meta-analyses in R with continuous variables (such as effect sizes), as these are the most common ones in psychology and the health science field. Later on, we will present briefly how to do meta-analyses with binary outcome data too, which might be important if you’re focusing on prevention trials. For these meta-analyses, we’ll use the metafor package (Viechtbauer and others 2010). In Section 2.1, we showed how to install the package. We load a different package: The metaforest package, which in turn loads the metafor package, and contains the example data for this tutorial. library(metaforest) References "],
["fixedef.html", "5.1 Fixed-Effects-Model", " 5.1 Fixed-Effects-Model The idea behind the fixed-effects-model The fixed-effects-model assumes that all observed effect sizes stem from a single true population effect (Borenstein et al. 2011). To calculate the overall effect, we therefore average all effect sizes, but give studies with greater precision a higher weight. Precision relates to the fact that studies with a larger N will provide more accurate estimates of the true population effect, as reflected by a smaller Standard Error of the effect size estimate. For this weighing, we use the inverse of the variance \\(1/\\hat\\sigma^2_k\\) of each study \\(k\\). We then calculate a weighted average of all studies, our fixed effect size estimator \\(\\hat\\theta_F\\): \\[\\begin{equation} \\hat\\theta_F = \\frac{\\sum\\limits_{k=1}^K \\hat\\theta_k/ \\hat\\sigma^2_k}{\\sum\\limits_{k=1}^K 1/\\hat\\sigma^2_k} \\end{equation}\\] First, let’s assume you already have a dataset with the calucated effects and SE for each study. The curry data set will do (you’ve used it before to practice loading data from Excel). This dataset has continuous outcome data. As our effect sizes are already calculated, we can directly use the rma function. For this function, we can specify loads of parameters, all of which you can accessed by typing ?rma in your console once the metafor package is loaded, or selecting the function and pressing F1. Here is a table with the most important parameters for our code: Parameter Function yi A vector with the effect sizes vi A vector with the sampling variances method A character string, indicating what type of meta-analysis to run. FE runs a fixed-effect model Let’s conduct our first fixed-effects-model Meta-Analysis. We we will give the results of this analysis the simple name m. m &lt;- rma(yi = df$d, # The d-column of the df, which contains Cohen&#39;s d vi = df$vi, # The vi-column of the df, which contains the variances method = &quot;FE&quot;) # Run a fixed-effect model m ## ## Fixed-Effects Model (k = 56) ## ## Test for Heterogeneity: ## Q(df = 55) = 156.9109, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## 0.2059 0.0219 9.4135 &lt;.0001 0.1630 0.2487 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We now see a summary of the results of our Meta-Analysis, including The total number of included studies (k) The overall effect (in our case, g = 0.2059) and its confidence interval and p-value The Q-test of heterogeneity The object m is a list: A collection of different types of data. A data.frame is a special kind of list, where every entry must have the same length. Using the $ command, we can look at parts of the list directly. For example, the estimates of heterogeneity: m$I2 ## [1] 64.94826 m$tau2 ## [1] 0 There are many ways to export results from R. The function sink() redirects output, which would normally be printed to the console, to a text file: sink(&quot;results.txt&quot;) print(m) sink() References "],
["random.html", "5.2 Random-Effects-Model", " 5.2 Random-Effects-Model We can only use the fixed-effect-model when we can assume that all included studies tap into one true effect size. In practice this is hardly ever the case: interventions may vary in certain characteristics, the sample used in each study might be slightly different, or its methods. A more appropriate assumption in these cases might be that the true effect size follows a normal distribution. The Idea behind the Random-Effects-Model In the Random-Effects-Model, we want to account for our assumption that the population effect size is normally distributed (Schwarzer, Carpenter, and Rücker 2015). The random-effects-model works under the so-called assumption of exchangeability. This means that in Random-Effects-Model Meta-Analyses, we not only assume that effects of individual studies deviate from the true intervention effect of all studies due to sampling error, but that there is another source of variance introduced by the fact that the studies do not stem from one single population, but are drawn from a “universe” of populations. We therefore assume that there is not only one true effect size, but a distribution of true effect sizes. We therefore want to estimate the mean and variance of this distribution of true effect sizes. The fixed-effect-model assumes that when the observed effect size \\(\\hat\\theta_k\\) of an individual study \\(k\\) deviates from the true effect size \\(\\theta_F\\), the only reason for this is that the estimate is burdened by (sampling) error \\(\\epsilon_k\\). \\[\\hat\\theta_k = \\theta_F + \\epsilon_k\\] While the random-effects-model assumes that, in addition, there is a second source of error \\(\\zeta_k\\).This second source of error is introduced by the fact that even the true effect size \\(\\theta_k\\) of our study \\(k\\) is also only part of an over-arching distribution of true effect sizes with the mean \\(\\mu\\) (Borenstein et al. 2011). An illustration of parameters of the random-effects-model The formula for the random-effects-model therefore looks like this: \\[\\hat\\theta_k = \\mu + \\epsilon_k + \\zeta_k\\] When calculating a random-effects-model meta-analysis, where therefore also have to take the error \\(\\zeta_k\\) into account. To do this, we have to estimate the variance of the distribution of true effect sizes, which is denoted by \\(\\tau^{2}\\), or tau2. There are several estimators for \\(\\tau^{2}\\), all of which are implemented in metafor. Even though it is conventional to use random-effects-model meta-analyses in psychological outcome research, applying this model is not undisputed. The random-effects-model pays more attention to small studies when pooling the overall effect in a meta-analysis (Schwarzer, Carpenter, and Rücker 2015). Yet, small studies in particular are often fraught with bias (see Chapter 8.1). This is why some have argued that the fixed-effects-model should be nearly always preferred (Poole and Greenland 1999; Furukawa, McGuire, and Barbui 2003). 5.2.1 Estimators for tau2 in the random-effects-model Operationally, conducting a random-effects-model meta-analysis in R is not so different from conducting a fixed-effects-model meta-analyis. Yet, we do have choose an estimator for \\(\\tau^{2}\\). Here are the estimators implemented in metafor, which we can choose using the method argument when calling the function. Code Estimator DL DerSimonian-Laird PM Paule-Mandel REML Restricted Maximum-Likelihood ML Maximum-likelihood HS Hunter-Schmidt SJ Sidik-Jonkman HE Hedges EB Empirical Bayes Which estimator should I use? All of these estimators derive \\(\\tau^{2}\\) using a slightly different approach, leading to somewhat different pooled effect size estimates and confidence intervals. If one of these approaches is more or less biased often depends on the context, and parameters such as the number of studies \\(k\\), the number of participants \\(n\\) in each study, how much \\(n\\) varies from study to study, and how big \\(\\tau^{2}\\) is. An overview paper by Veroniki and colleagues (Veroniki et al. 2016) provides an excellent summary on current evidence which estimator might be more or less biased in which situation. The article is openly accessible, and you can read it here. This paper suggests that the Restricted Maximum-Likelihood estimator performs best, and it is the default estimator in metafor. 5.2.2 Conducting the analysis Random-effects meta-analyses are very easy to code in R. Compared to the fixed-effects-model Chapter 5.1, we can simply remove the method = &quot;FE&quot; argument, if we want to use the default REML estimator: m_re &lt;- rma(yi = df$d, # The d-column of the df, which contains Cohen&#39;s d vi = df$vi) # The vi-column of the df, which contains the variances m_re ## ## Random-Effects Model (k = 56; tau^2 estimator: REML) ## ## tau^2 (estimated amount of total heterogeneity): 0.0570 (SE = 0.0176) ## tau (square root of estimated tau^2 value): 0.2388 ## I^2 (total heterogeneity / total variability): 67.77% ## H^2 (total variability / sampling variability): 3.10 ## ## Test for Heterogeneity: ## Q(df = 55) = 156.9109, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## 0.2393 0.0414 5.7805 &lt;.0001 0.1581 0.3204 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The output shows that our estimated effect is \\(g=0.2393\\), and the 95% confidence interval stretches from \\(g=0.16\\) to \\(0.32\\) (rounded). The estimated heterogeneity is \\(\\tau^2 = 0.06\\). The percentage of variation across effect sizes that is due to heterogeneity rather than change is estimated at \\(I^2 = 67.77\\%\\). The summary effect estimate is different (and larger) than the one we found in the fixed-effects-model meta-analysis in Chapter 5.1 (\\(g=0.21\\)). References "],
["forest-plots.html", "Chapter 6 Forest Plots", " Chapter 6 Forest Plots Now that we created the output of our meta-analysis using the rma function in metafor (see Chapter 5.1, and Chapter 5.2), it is time to present the data in a more digestable way. Forest Plots are an easy way to do this, and it is conventional to report forest plots in meta-analysis publications. "],
["generating-a-forest-plot.html", "6.1 Generating a Forest Plot", " 6.1 Generating a Forest Plot To produce a forest plot, we use the meta-analysis output we just created (e.g., m, m_re) and apply the rma::forest() function. forest(m_re, slab = df$study_id) 6.1.1 Prediction interval Prediction intervals give us a range for which we can expect the effects of future studies to fall based on our present evidence in the meta-analysis. They take the between-study variance into account. If our prediction interval, for example, lies completely on the positive side favoring the intervention, we can be quite confident to say that despite varying effects, the intervention might be at least in some way beneficial in all contexts we studied in the future. If the confidence interval includes zero, we can be less sure about this, although it should be noted that broad prediction intervals are quite common, especially in medicine and psychology. We can simply add a prediction interval to the forest plot: forest(m_re, slab = df$study_id, addcred = TRUE) "],
["saving-the-forest-plot.html", "6.2 Saving the forest plot", " 6.2 Saving the forest plot Let’s say I want to save the Forest Plot now. The easiest way to do this is to plot it to a graphics device instead of to the screen. Just like the function sink() redirected text output from the console tab to a text file, there are functions that redirect images from the plot tab to a file. One of these functions is pdf(), which opens the PDF graphics device. You can then plot your image, it will be sent to the PDF, and then close the device again. This saves the plot into a PDF to the Working Directory. This way, you can export the plot in different formats (you can find more details on the saving options here). PDF pdf(file=&#39;forestplot.pdf&#39;) # Open PDF device with specific file name forest(m_re, slab = df$study_id) # Plot the forest dev.off() # Turn the PDF device off PNG png(file=&#39;forestplot.png&#39;) # Open PNG device with specific file name forest(m_re, slab = df$study_id) # Plot the forest dev.off() Scalable Vector Graphic svg(file=&#39;forestplot.svg&#39;) # Open SVG device with specific file name forest(m_re, slab = df$study_id) # Plot the forest dev.off() "],
["heterogeneity.html", "Chapter 7 Between-study Heterogeneity", " Chapter 7 Between-study Heterogeneity By now, we have already shown you how to pool effect sizes in a meta-analysis. In meta-analytic pooling, we aim to synthesize the effects of many different studies into one single effect. However, this makes only sense if we aren’t comparing Apples and Oranges. For example, it could be the case that while the overall effect we calculate in the meta-analysis is small, there are still a few studies which report very high effect sizes. Such information is lost in the aggregate effect, but it is very important to know if all studies, or interventions, yield small effect sizes, or if there are exceptions. It could also be the case that even some very extreme effect sizes were included in the meta-analysis, so-called outliers. Such outliers might have even distorted our overall effect, and it is important to know how our overall effect would have looked without them. The extent to which effect sizes vary within a meta-analysis is called heterogeneity. It is very important to assess heterogeneity in meta-analyses, as high heterogeneity could be caused by between-studies differences. For example, a continuous variable, like the proportion of male participants, might influence the effect - or there might be two or more subgroups of studies present in the data, which have a different true effect. Such information could be very valuable for research, because this might allow us to find certain interventions or populations for which effects are lower or higher. Very high heterogeneity could even mean that the studies have nothing in common, and that there is no “real” true effect behind our data, meaning that it makes no sense to report the pooled effect at all (Borenstein et al. 2011). References "],
["heterogeneity-statistics.html", "7.1 Heterogeneity statistics", " 7.1 Heterogeneity statistics There are three types of heterogeneity measures which are commonly used to assess the degree of heterogeneity. In the following examples, \\(k\\) denotes the individual study, \\(K\\) denotes all studies in our meta-analysis, \\(\\hat \\theta_k\\) is the estimated effect of \\(k\\) with a variance of \\(\\hat \\sigma^{2}_k\\), and \\(w_k\\) is the individual weight of the study (i.e., its inverse variance: \\(w_k = \\frac{1}{\\hat \\sigma^{2}_k}\\); see infobox in Chapter 5.1.1 for more details). 1. Cochran’s Q Cochran’s Q-statistic is the difference between the observed effect sizes and the fixed-effect model estimate of the effect size, which is then squared, weighted and summed (a sort of weighted standard deviation around the fixed-effect summary effect). \\[ Q = \\sum\\limits_{k=1}^K w_k (\\hat\\theta_k - \\frac{\\sum\\limits_{k=1}^K w_k \\hat\\theta_k}{\\sum\\limits_{k=1}^K w_k})^{2}\\] 2. Higgin’s &amp; Thompson’s I2 \\(I^{2}\\) (Higgins and Thompson 2002) is the percentage of variability in the effect sizes which is not caused by sampling error. It is derived from \\(Q\\): \\[I^{2} = max \\left\\{0, \\frac{Q-(K-1)}{Q} \\right\\}\\] 3. Tau-squared \\(\\tau^{2}\\) is the between-study variance in our meta-analysis. It is an estimate of the variance of the underlying distribution of true effect sizes. As we show in Chapter 5.2.1, there are various proposed ways to calculate \\(\\tau^{2}\\). Which measure should i use? Generally, when we assess and report heterogeneity in a meta-analysis, we need a measure which is robust, and not to easily influenced by statistical power. Cochran’s Q increases both when the number of studies (\\(k\\)) increases, and when the precision (i.e., the sample size \\(N\\) of a study) increases. Therefore, \\(Q\\) and weather it is significant highly depends on the size of your meta-analysis, and thus its statistical power. We should therefore not only rely on \\(Q\\) when assessing heterogeneity. I2 on the other hand, is not sensitive to changes in the number of studies in the analyses. \\(I^2\\) is therefore used extensively in medical and psychological research, especially since there is a “rule of thumb” to interpret it (Higgins et al. 2003): I2 = 25%: low heterogeneity I2 = 50%: moderate heterogeneity I2 = 75%: substantial heterogeneity Despite its common use in the literature, \\(I^2\\) not always an adequate measure for heterogeneity either, because it still heavily depends on the precision of the included studies (Rücker et al. 2008; Borenstein et al. 2017). As said before, \\(I^{2}\\) is simply the amount of variability not caused by sampling error. If our studies become increasingly large, this sampling error tends to zero, while at the same time, \\(I^{2}\\) tends to 100% simply because the single studies have greater \\(N\\). Only relying on \\(I^2\\) is therefore not a good option either. Tau-squared, on the other hand, is insensitive to the number of studies, and the precision. Yet, it is often hard to interpret how relevant our tau-squared is from a practical standpoint. References "],
["assessing-the-heterogeneity-of-your-pooled-effect-size.html", "7.2 Assessing the heterogeneity of your pooled effect size", " 7.2 Assessing the heterogeneity of your pooled effect size Thankfully, once you’ve already pooled your effects in meta-analysis using the metafor() function, it is very easy and straightforward to retrieve the three most common heterogeneity measures that we described before. In Chapter 5.2, we already showed you how to conduct a random-effect-model meta-analysis. In this example, we stored our results in the object m_re, which we will use again here. One way to get heterogeneity measures of my meta-analysis is to print the meta-analysis (in my case, m_re) output again. Just running the name of an object calls the print() method for that object; notice that these two commands do the same thing: m_re ## ## Random-Effects Model (k = 56; tau^2 estimator: REML) ## ## tau^2 (estimated amount of total heterogeneity): 0.0570 (SE = 0.0176) ## tau (square root of estimated tau^2 value): 0.2388 ## I^2 (total heterogeneity / total variability): 67.77% ## H^2 (total variability / sampling variability): 3.10 ## ## Test for Heterogeneity: ## Q(df = 55) = 156.9109, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## 0.2393 0.0414 5.7805 &lt;.0001 0.1581 0.3204 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 print(m_re) ## ## Random-Effects Model (k = 56; tau^2 estimator: REML) ## ## tau^2 (estimated amount of total heterogeneity): 0.0570 (SE = 0.0176) ## tau (square root of estimated tau^2 value): 0.2388 ## I^2 (total heterogeneity / total variability): 67.77% ## H^2 (total variability / sampling variability): 3.10 ## ## Test for Heterogeneity: ## Q(df = 55) = 156.9109, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## 0.2393 0.0414 5.7805 &lt;.0001 0.1581 0.3204 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that this output already provides us with all three heterogeneity measures (and even one more, H^2, which we will not cover here). \\(\\tau^{2}\\), as we can see from the tau^2 output, is 0.0570. \\(I^{2}\\) is printed next to I^2, and has the value 67.77%. The value of \\(Q\\) is displayed next to Q under Test for heterogeneity:. As we can see, the value is 156.9109. In our case, this is significant (\\(p &lt; 0.001\\); see p-value). We can also obtain a prediction interval, which we plotted in the forest plot, by running predict(m_re). As we can see, the 95% prediction interval (cr.lb and cr.ub) ranges from g=-0.2358 to 0.7143. It is calculated simply as \\(\\hat{\\mu} \\pm 1.96 * \\hat{\\tau}\\): The estimated summary effect, with a 95% window based on the estimated standard deviation of the true effect. How can we interpret the values of this example analysis? Well, all of our indicators suggest that moderate to substantial heterogeneity is present in our data. Given the broad prediction interval, which stretches well below zero, we also cannot be overly confident that the positive effect we found for our interventions is robust in every context. It might be very well possible that the intervention does not yield positive effects in some future scenarios; even a small negative effect might be possible based on the evidence the meta-analysis gives us. Very high effect sizes, on the other hand, are possible too. "],
["detecting-outliers-influential-cases.html", "7.3 Detecting outliers &amp; influential cases", " 7.3 Detecting outliers &amp; influential cases As mentioned before, between-study heterogeneity can also be caused by one more studies with extreme effect sizes which don’t quite fit in. Especially when the quality of these studies is low, or the studies are very small, this may distort our pooled effect estimate, and it’s a good idea to have a look on the pooled effect again once we remove such outliers from the analysis. On the other hand, we also want to know if the pooled effect estimate we found is robust, meaning that the effect does not depend heavily on one single study. Therefore, we also want to know whether there are studies which heavily push the effect of our analysis into some direction. Such studies are called influential cases, and we’ll devote some time to this topic in the second part of this chapter. It should be noted that they are many methods to spot outliers and influential cases, and the methods described here are not comprehensive. If you want to read more about the underpinnings of this topic, we can recommend the paper by Wolfgang Viechtbauer and Mike Cheung (Viechtbauer and Cheung 2010). 7.3.1 Searching for extreme effect sizes (outliers) A common method to detect outliers directly is to define a study as an outlier if the study’s confidence interval does not overlap with the confidence interval of the pooled effect. To detect such outliers in our dataset, the filter function in the dplyr package we introduced in Chapter 3.3.3 comes in handy again. Using this function, we can search for all studies: for which the upper bound of the 95% confidence interval of the study is lower than the lower bound of the pooled effect confidence interval (i.e., extremely small effects) for which the lower bound of the 95% confidence interval of the study is higher than the higher bound of the pooled effect confidence interval (i.e., extremely large effects) Here, i’ll use my m_re meta-analysis output from Chapter 5.2.2 again. Let’s see what the upper and lower bound of the pooled effect confidence interval are: m_re$ci.lb ## [1] 0.1581353 m_re$ci.ub ## [1] 0.3203839 The pooled effect confidence interval stretches from \\(g = 0.16\\) to \\(g = 0.32\\). We can use these values to filter out outliers now. To filter out outliers, we will use a boolean (TRUE/FALSE) filter variable. We first calculate the 95% CI for each study effect size, using the standard error of the effect size (sqrt(vi)). Then, we create a new filter variable outlier, with the value TRUE if the CI of the effect size is outside of the CI of the pooled effect. We use some logical operators here: &lt;, &gt;, and |. The first two mean “smaller than” and “bigger than”, and are probably familiar. THe third one, |, means “or”. So the statement df$upperci &lt; m_re$ci.lb | df$lowerci &gt; m_re$ci.ub means: The upper CI bound of the effect is smaller than the lower CI bound of the pooled effect, OR the lower CI bound of the effect is bigger than the upper CI bound of the pooled effect. # Calculate CI for all observed effect sizes df$upperci &lt;- df$d + 1.96 * sqrt(df$vi) df$lowerci &lt;- df$d - 1.96 * sqrt(df$vi) # Create filter variable df$outlier &lt;- df$upperci &lt; m_re$ci.lb | df$lowerci &gt; m_re$ci.ub # Count number of outliers: sum(df$outlier) ## [1] 8 We see that there are eight potential outliers. Let’s examine the effect sizes of these outliers. We can use the df$outlier variable to select only the filtered outliers, and we can request the effect size-related variables to get some idea for whether the outliers are mostly on the positive or negative side: # Look at effect sizes for potential outliers df[df$outlier, c(&quot;d&quot;, &quot;upperci&quot;, &quot;lowerci&quot;)] Based on this output, it’s hard to determine if the outliers might bias the estimate. Let’s get a graphical representation of the histogram for the effect sizes, with different colours for the flagged outliers. For this, we will use the popular plotting package ggplot2, which can build any possible plot in cumulative steps: # Load ggplot library(ggplot2) # Make a basic plot, based on the data in df, and specify that the x-variable is # the effect size, &#39;d&#39;, the colour and fill of the histogram bars are based on # the value of &#39;outlier&#39;: ggplot(data = df, aes(x = d, colour = outlier, fill = outlier)) + # Add a histogram with transparent bars (alpha = .2) geom_histogram(alpha = .2) + # Add a vertical line at the pooled effect value (m_re$b[1]) geom_vline(xintercept = m_re$b[1]) + # Apply a black and white theme theme_bw() ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. It looks like the potential flagged outliers are pretty uniformly distributed. Thus, there is no clear indication of bias. Note that we can plot essentially anything using ggplot, so it’s an extremely useful package for understanding your data visually. 7.3.2 Sensitivity analysis We can also do a sensitivity analysis, to check how much the pooled effect size changes if we omit all potential outliers: m_no_outliers &lt;- rma(yi = df$d[!df$outlier], vi = df$vi[!df$outlier]) # Print the rounded pooled effect and CI cat(&quot;G = &quot;, round(m_no_outliers$b[1], 2), &quot;, 95% CI [&quot;, round(m_no_outliers$ci.lb, 2), &quot;, &quot;, round(m_no_outliers$ci.ub, 2), &quot;] (no outliers)&quot;, sep = &quot;&quot;) ## G = 0.22, 95% CI [0.17, 0.27] (no outliers) cat(&quot;G = &quot;, round(m_re$b[1], 2), &quot;, 95% CI [&quot;, round(m_re$ci.lb, 2), &quot;, &quot;, round(m_re$ci.ub, 2), &quot;]&quot;, sep = &quot;&quot;) ## G = 0.24, 95% CI [0.16, 0.32] Again, the conclusion is that these outliers hardly bias the pooled effect. References "],
["subgroup.html", "Chapter 8 Subgroup Analyses", " Chapter 8 Subgroup Analyses In Chapter 6, we discussed in depth why between-study heterogeneity is such an important issue in interpreting the results of our meta-analysis, and how we can explore the role of outliers as potential sources of heterogeneity. Another source of between-study heterogeneity making our effect size estimate less precise could be that there are known differences between studies. For example, in a meta-analysis on the effects of cognitive behavioral therapy (CBT) for depression in university students, it could be the case that some studies delivered the intervention in a group setting, while others delivered the therapy to each student individually. In the same example, it is also possible that studies used different criteria to determine if a student suffers from depression (e.g. they either used the ICD-10 or the DSM-5 diagnostic manual). Many other differences of this sort are possible, and such study differences could cause differences in the overall effect. We can control for the influence of such factors using various forms of meta-regression: Analyses that control for the influence of between-study differences. So-called subgroup analyses are similar to an ANOVA, in that they are a regression analysis with only one categorical predictor. We can use them to look at different subgroups within the studies of our meta-analysis and try to determine the extent of the difference between these subgroups. The idea behind subgroup analyses Basically, a every subgroup analysis consists of two parts: (1) pooling the effect of each subgroup, and (2) comparing the effects of the subgroups (Borenstein and Higgins 2013). 1. Pooling the effect of each subgroup This point it rather straightforward, as the same criteria as the ones for a simple meta-analysis without subgroups (see Chapter 4 and Chapter 4.2) apply here. If you assume that all studies in subgroup stem from the same population, and all have one shared true effect, you may use the fixed-effect-model. As we mention in Chapter 4, many doubt that this assumption is ever true in psychological and medical research, even when we partition our studies into subgroups. The alternative, therefore, is to use a random-effect-model which assumes that the studies within a subgroup are drawn from a universe of populations follwing its own distribution, for which we want to estimate the mean. 2. Comparing the effects of the subgroups After we calculated the pooled effect for each subgroup, we can compare the size of the effects of each subgroup. However, to know if this difference is in fact singnificant and/or meaningful, we have to calculate the Standard Error of the differences between subgroup effect sizes \\(SE_{diff}\\), to calculate confidence intervals and conduct significance tests. There are two ways to calculate \\(SE_{diff}\\), and both based on different assumptions. Fixed-effects (plural) model: The fixed-effects-model for subgroup comparisons is appropriate when we are only interested in the subgroups at hand (Borenstein and Higgins 2013). This is the case when the subgroups we chose to examine were not randomly “chosen”, but represent fixed levels of a characteristic we want to examine. Gender is such a characteristic, as its two subgroups female and male were not randomly chosen, but are the two subgroups that gender (in its classical conception) has. Same does also apply, for example, if we were to examine if studies in patients with clinical depression versus subclinical depression yield different effects. Borenstein and Higgins (@ Borenstein and Higgins 2013) argue that the fixed-effects (plural) model may be the only plausible model for most analysis in medical research, prevention, and other fields. As this model assumes that no further sampling error is introduced at the subgroup level (because subgroups were not randomly sampled, but are fixed), \\(SE_{diff}\\) only depends on the variance within the subgroups \\(A\\) and \\(B\\), \\(V_A\\) and \\(V_B\\). \\[V_{Diff}=V_A + V_B\\] The fixed-effects (plural) model can be used to test differences in the pooled effects between subgroups, while the pooling within the subgroups is still conducted using a random-effects-model. Such a combination is sometimes called a mixed-effects-model. We’ll show you how to use this model in R in the next chapter. Random-effects-model: The random-effects-model for between-subgroup-effects is appropriate when the subgroups we use were randomly sampled from a population of subgroups. Such an example would be if we were interested if the effect of an intervention varies by region by looking at studies from 5 different countries (e.g., Netherlands, USA, Australia, China, Argentina). These variable “region” has many different potential subgroups (countries), from which we randomly selected five means that this has introduced a new sampling error, for which we have to control for using the random-effects-model for between-subgroup-comparisons. The (simplified) formula for the estimation of \\(V_{Diff}\\) using this model therefore looks like this: \\[V_{Diff}=V_A + V_B + \\frac{\\hat T^2_G}{m} \\] Where \\(\\hat T^2_G\\) is the estimated variance between the subgroups, and \\(m\\) is the number of subgroups. Be aware that subgroup analyses should always be based on an informed, a priori decision which subgroup differences within the study might be practically relevant, and would lead to information gain on relevant research questions in your field of research. It is also good practice to specify your subgroup analyses before you do the analysis, and list them in the registration of your analysis. It is also important to keep in mind that the capabilites of subgroup analyses to detect meaningful differences between studies is often limited. Subgroup analyses also need sufficient power, so it makes no sense to compare two or more subgroups when your entire number of studies in the meta-analysis is smaller than \\(k=10\\) (Higgins and Thompson 2004). References "],
["mixed.html", "8.1 Mixed-Effects-Model", " 8.1 Mixed-Effects-Model To conduct subgroup analyses using the Mixed-Effects-Model (random-effects-model within subgroups, fixed-effects-model between subgroups), you can simply include your grouping variable as a categorical predictor in the rma function. Like a classic t-test or ANOVA or regression model, this approach assumes homoscedasticity: The residual heterogeneity is assumed to be the same across groups. We can use subgroup analysis to examine whether there are significant differences in the pooled effect between studies conducted within the USA, and studies conducted elsewhere. To do so, we first make a new variable, Country, that codes for studies conducted (partly) within the USA. Then, we specify the mixed-effects model with ~Country as a moderator. We drop the intercept by specifying -1, so the model estimates the mean effect size for both groups (as in ANOVA): # Create a factor (categorical) variable for location df$Country &lt;- factor(df$location %in% c(&quot;USA&quot;, &quot;USA/Korea&quot;), labels = c(&quot;Elsewhere&quot;, &quot;USA&quot;)) m_subgroup &lt;- rma(yi = d, vi = vi, mods = ~ Country-1, data = df) m_subgroup ## ## Mixed-Effects Model (k = 56; tau^2 estimator: REML) ## ## tau^2 (estimated amount of residual heterogeneity): 0.0503 (SE = 0.0164) ## tau (square root of estimated tau^2 value): 0.2243 ## I^2 (residual heterogeneity / unaccounted variability): 64.66% ## H^2 (unaccounted variability / sampling variability): 2.83 ## ## Test for Residual Heterogeneity: ## QE(df = 54) = 138.1105, p-val &lt; .0001 ## ## Test of Moderators (coefficient(s) 1:2): ## QM(df = 2) = 39.6824, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## CountryElsewhere 0.1582 0.0572 2.7635 0.0057 0.0460 0.2704 ** ## CountryUSA 0.3132 0.0553 5.6609 &lt;.0001 0.2048 0.4216 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that the pooled effects of the subgroups differ quite substantially: \\(g = .3132\\) for studies conducted in the USA, and \\(g = .1582\\) for studies conducted elsewhere. But is the difference statistically significant? There are two ways we can find out. 8.1.1 Regression specification We can test for the significance of the difference between groups by re-specifying the model using the regression specification: With an intercept, and an effect for the dummy variable Country, which is the difference between the two groups: # Re-specify the model with an intercept and dummy m_dummy &lt;- rma(yi = d, vi = vi, mods = ~ Country, data = df) m_dummy ## ## Mixed-Effects Model (k = 56; tau^2 estimator: REML) ## ## tau^2 (estimated amount of residual heterogeneity): 0.0503 (SE = 0.0164) ## tau (square root of estimated tau^2 value): 0.2243 ## I^2 (residual heterogeneity / unaccounted variability): 64.66% ## H^2 (unaccounted variability / sampling variability): 2.83 ## R^2 (amount of heterogeneity accounted for): 11.80% ## ## Test for Residual Heterogeneity: ## QE(df = 54) = 138.1105, p-val &lt; .0001 ## ## Test of Moderators (coefficient(s) 2): ## QM(df = 1) = 3.7903, p-val = 0.0515 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## intrcpt 0.1582 0.0572 2.7635 0.0057 0.0460 0.2704 ** ## CountryUSA 0.1550 0.0796 1.9469 0.0515 -0.0010 0.3110 . ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The results indicate that the difference between the pooled effect sizes of studies conducted in the USA and elsewhere is \\(\\Delta g = 0.16, z = 1.95, p = .052\\): Not significant. 8.1.2 T-test on the coefficients Another way to test the significance of the difference is by manually conducting a post-hoc t-test on the two means from the model with ANOVA specification. The metaforest package contains a convenience function that conducts t-tests or z-tests on the parameters of rma meta-analyses. Now, we can use the function to conduct the t-test: coef_test(m_subgroup, &quot;CountryUSA&quot;, &quot;CountryElsewhere&quot;) ## t(54) = 1.95, p = 0.06 The p-value is a bit higher, but otherwise the result is the same as the effect of the dummy variable in m_dummy. The difference in p-values is because we calculated a t-test here, whereas metafor uses z-tests by default. 8.1.3 Free variance per group A logical question might be: Is it a reasonable assumption that the variance for studies conducted within the USA is equal to that of studies conducted elsewhere? After all - studies conducted elsewhere could come from all over the world, so they might be more heterogeneous. It is possible to free the variance between the subgroups, but this is quite advanced. There is an excellent tutorial on it online, if you need to do this in your future work: http://www.metafor-project.org/doku.php/tips:comp_two_independent_estimates "],
["metareg.html", "Chapter 9 Meta-Regression", " Chapter 9 Meta-Regression Conceptually, Meta-Regression does not differ much from a subgroup analysis. In fact, subgroup analyses with more than two groups are nothing more than a meta-regression with categorial covariates. Meta-regression with continuous moderators reveals whether values of this continuous variable are linearly associated with the effect size. The idea behind meta-regression In a conventional regression, we specify a model predicting the dependent variable \\(y\\), across \\(_i\\) participants, based on their values on \\(p\\) predictor variables, \\(x_{i1} \\dots x_{ip}\\). The residual error is referred to as \\(\\epsilon_i\\). A standard regression equation therefore looks like this: \\[y_i=\\beta_0 + \\beta_1x_{1i} + ...+\\beta_px_{pi} + \\epsilon_i\\] In a meta-regression, we want to estimate the effect size \\(\\theta\\) of several studies \\(_k\\), as a function of between-studies moderators. There are two sources of heterogeneity: sampling error, \\(\\epsilon_k\\), and between-studies heterogeneity, \\(\\zeta_k\\) so our regression looks like this: \\[\\theta_k = \\beta_0 + \\beta_1x_{1k} + ... + \\beta_px_{pk} + \\epsilon_k + \\zeta_k\\] You might have seen that when estimating the effect size \\(\\theta_k\\) of a study \\(k\\) in our regression model, there are two extra terms in the equation, \\(\\epsilon_k\\) and \\(\\zeta_k\\). The same terms can also be found in the equation for the random-effects-model in Chapter 4.2. The two terms signify two types of independent errors which cause our regression prediction to be imperfect. The first one, \\(\\epsilon_k\\), is the sampling error through which the effect size of the study deviates from its “true” effect. The second one, \\(\\zeta_k\\), denotes that even the true effect size of the study is only sampled from an overarching distribution of effect sizes (see the Chapter on the Random-Effects-Model for more details). In a fixed-effect-model, we assume that all studies actually share the same true effect size and that the between-study heterogeneity \\(\\tau^2 = 0\\). In this case, we do not consider \\(\\zeta_k\\) in our equation, but only \\(\\epsilon_k\\). As the equation above has includes fixed effects (the \\(\\beta\\) coefficients) as well as random effects (\\(\\zeta_k\\)), the model used in meta-regression is often called a mixed-effects-model. Mathematically, this model is identical to the mixed-effects-model we described in Chapter 7 where we explained how subgroup analyses work. Indeed subgroup analyses are nothing else than a meta-regression with a categorical predictor. For meta-regression, these subgroups are then dummy-coded, e.g. \\[ D_k = \\{\\begin{array}{c}0:ACT \\\\1:CBT \\end{array}\\] \\[\\hat \\theta_k = \\theta + \\beta x_{k} + D_k \\gamma + \\epsilon_k + \\zeta_k\\] In this case, we assume the same regression line, which is simply “shifted” up or down for the different subgroups \\(D_k\\). Figure 9.1: Visualisation of a Meta-Regression with dummy-coded categorial predictors Assessing the fit of a regression model To evaluate the statistical significance of a predictor, we conduct a t-test of its \\(\\beta\\)-weight. \\[ t=\\frac{\\beta}{SE_{\\beta}}\\] Which provides a \\(p\\)-value telling us if a variable significantly predicts effect size differences in our regression model. If we fit a regression model, our aim is to find a model which explains as much as possible of the current variability in effect sizes we find in our data. In conventional regression, \\(R^2\\) is commonly used to quantify the percentage of variance in the data explained by the model, as a percentage of total variance (0-100%). As this measure is commonly used, and many researchers know how to to interpret it, we can also calculate a \\(R^2\\) analog for meta-regression using this formula: \\[R_2=\\frac{\\hat\\tau^2_{REM}-\\hat\\tau^2_{MEM}}{\\hat\\tau^2_{REM}}\\] Where \\(\\hat\\tau^2_{REM}\\) is the estimated total heterogenetiy based on the random-effects-model and \\(\\hat\\tau^2_{REM}\\) the total heterogeneity of our mixed-effects regression model. NOTE however, that \\(R^2\\) refers to variance explained in the observed data. The more predictors you add, the better your model will explain the observed data. But this can decrease the generalizability of you model, through a process called overfitting: You’re capturing noise in your dataset, not true effects that exist in the real world. "],
["meta-regression-in-r.html", "9.1 Meta-regression in R", " 9.1 Meta-regression in R Meta-regressions can be conducted in R using the rma function in metafor. To show the similarity between subgroup analysis and meta-regression, consider the code for our regression-specified subgroup analysis again: m_dummy &lt;- rma(yi = d, vi = vi, mods = ~ Country, data = df) This syntax used “country” as a dummy-coded variable. R will always do this for factor variables. It is possible to drop the intercept and estimate the means for all groups instead by specifying ~Country-1, and it is even possible to change the default dummy coding if you want to compare a set of groups with another set of groups: a tutorial. Continuous variables Imagine you want to check if the proportion of male participants is associated with effect size. The variable sex contains this information. You can use this predictor in a meta-regression: m_reg &lt;- rma(yi = d, vi = vi, mods = ~sex, data = df) m_reg ## ## Mixed-Effects Model (k = 56; tau^2 estimator: REML) ## ## tau^2 (estimated amount of residual heterogeneity): 0.0544 (SE = 0.0173) ## tau (square root of estimated tau^2 value): 0.2333 ## I^2 (residual heterogeneity / unaccounted variability): 66.50% ## H^2 (unaccounted variability / sampling variability): 2.98 ## R^2 (amount of heterogeneity accounted for): 4.53% ## ## Test for Residual Heterogeneity: ## QE(df = 54) = 149.5878, p-val &lt; .0001 ## ## Test of Moderators (coefficient(s) 2): ## QM(df = 1) = 2.1607, p-val = 0.1416 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## intrcpt 0.0648 0.1253 0.5168 0.6053 -0.1808 0.3104 ## sex 0.0050 0.0034 1.4699 0.1416 -0.0017 0.0116 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 As you can see from the output, sex was now included as a predictor, but it is not significantly associated with the effect size (\\(p=.1416\\)). "],
["plotting-regressions.html", "9.2 Plotting regressions", " 9.2 Plotting regressions To plot our meta-regression output, we can make a bubble plot using ggplot. This is essentially a weighted scatter plot, where the size of the scatter is mapped to the inverse SE of each effect size, which means the area of the scatter is proportional to the inverse variance: library(ggplot2) df$weights &lt;- 1/sqrt(df$vi) # Specify basic plot, mapping sex to the x-axis, effect size &#39;d&#39; to the y-axis, # and &#39;weights&#39; to the weight parameter. ggplot(df, aes(x = sex, y = d, size = weights)) + geom_point(shape = 1) + # Add scatter geom_abline(intercept = m_reg$b[1], slope = m_reg$b[2]) + # Add regression line theme_bw() + # Apply black and white theme theme(legend.position = &quot;none&quot;) # Remove legend "],
["multiple-meta-regression.html", "9.3 Multiple Meta-Regression", " 9.3 Multiple Meta-Regression Previously, we only considered the scenario in which we use one predictor \\(\\beta_1x_1\\) in our meta-regression. When we add more than one predictor, we’re using multiple meta-regression. In multiple meta-regression we use several moderators (variables) to predict the outcome (effect sizes). When we look back at the general meta-regression formula we defined before, we actually see that the formula already provides us with this feature through the \\(\\beta_nx_{pk}\\) part. Here, the parameter \\(p\\) denotes that we can include \\(p\\) more predictors/variables into our meta-regression, making it a multiple meta-regression. Imagine, for example, that we expect the effect size to be determined by sex, but also by the type of outcome variable that was measured, outcomecode. We could include both predictors in a mutliple meta-regression as follows: m_multi &lt;- rma(yi = d, vi = vi, mods = ~sex+outcomecode, data = df) m_multi ## ## Mixed-Effects Model (k = 56; tau^2 estimator: REML) ## ## tau^2 (estimated amount of residual heterogeneity): 0.0595 (SE = 0.0190) ## tau (square root of estimated tau^2 value): 0.2440 ## I^2 (residual heterogeneity / unaccounted variability): 67.54% ## H^2 (unaccounted variability / sampling variability): 3.08 ## R^2 (amount of heterogeneity accounted for): 0.00% ## ## Test for Residual Heterogeneity: ## QE(df = 51) = 148.3124, p-val &lt; .0001 ## ## Test of Moderators (coefficient(s) 2:5): ## QM(df = 4) = 2.2088, p-val = 0.6974 ## ## Model Results: ## ## estimate se zval pval ci.lb ## intrcpt 0.0880 0.1491 0.5903 0.5550 -0.2042 ## sex 0.0049 0.0035 1.4148 0.1571 -0.0019 ## outcomecodeLife Satisfaction -0.0625 0.1543 -0.4051 0.6854 -0.3649 ## outcomecodeOther -0.0318 0.1272 -0.2500 0.8026 -0.2811 ## outcomecodePN Affect -0.0169 0.1121 -0.1510 0.8800 -0.2365 ## ci.ub ## intrcpt 0.3801 ## sex 0.0118 ## outcomecodeLife Satisfaction 0.2399 ## outcomecodeOther 0.2175 ## outcomecodePN Affect 0.2027 ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We will discuss a few important pitfalls in multiple meta-regression and how we can build multiple meta-regression models which are robust and trustworthy. But first, let’s cover another important feature of multiple meta-regression: interactions. "],
["interactions.html", "9.4 Interactions", " 9.4 Interactions So far, in our multiple meta-regression model, we only considered the case where we have multiple predictor variables \\(x_1,x_2, ... x_p\\), and along with their predictor estimates \\(\\beta_p\\), add them together to calculate our estimate of the true effect size \\(\\hat \\theta_k\\) for each study \\(k\\). In multiple meta-regression models, however, we can not only model such additive relationships. We can also model so-called interactions. Interactions mean that the relationship between one predictor variable (e.g., \\(x_1\\)) and the estimated effect size is different for different values of another predictor variable (e.g. \\(x_2\\)). Imagine a scenario where we want to model two predictors and their relationship to the effect size: the sex (\\(x_1\\)) of participants and the donor type (\\(x_2\\)) of participants in a study (either Anxious or Typical). As we described before, we can now imagine a meta-regression model in which we combine these two predictors \\(x_1\\) and \\(x_2\\) and assume an additive relationship. We can do this by simply adding them: \\[\\theta_k = \\beta_0 + \\beta_1x_{1k} + \\beta_2x_{2k} + \\epsilon_k + \\zeta_k\\] We could now ask ourselves if the relationship of participants’ sex depends on the type of participants (Anxious or Typical; \\(x_2\\)). For example, maybe sex predicts effect size more strongly in Anxious populations? To examine such questions, we can add an interaction term to our meta-regression model. This interaction term lets predictions of \\(x_1\\) vary for different values of \\(x_2\\) (and vice versa). We can denote this additional interactional relationship in our model by introducing a third predictor, \\(\\beta_3\\), which captures this interaction \\(x_{1k}x_{2k}\\) we want to test in our model: \\[\\theta_k = \\beta_0 + \\beta_1x_{1k} + \\beta_2x_{2k} + \\beta_3x_{1k}x_{2k}+ \\epsilon_k + \\zeta_k\\] The R-syntax for this interaction is: m_int &lt;- rma(yi = d, vi = vi, mods = ~sex*donorcode, data = df) m_int ## ## Mixed-Effects Model (k = 56; tau^2 estimator: REML) ## ## tau^2 (estimated amount of residual heterogeneity): 0.0537 (SE = 0.0174) ## tau (square root of estimated tau^2 value): 0.2318 ## I^2 (residual heterogeneity / unaccounted variability): 66.54% ## H^2 (unaccounted variability / sampling variability): 2.99 ## R^2 (amount of heterogeneity accounted for): 5.79% ## ## Test for Residual Heterogeneity: ## QE(df = 52) = 144.6429, p-val &lt; .0001 ## ## Test of Moderators (coefficient(s) 2:4): ## QM(df = 3) = 5.0950, p-val = 0.1650 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## intrcpt -5.4291 3.8294 -1.4177 0.1563 -12.9345 2.0764 ## sex 0.2016 0.1399 1.4402 0.1498 -0.0727 0.4759 ## donorcodeTypical 5.5351 3.8316 1.4446 0.1486 -1.9747 13.0450 ## sex:donorcodeTypical -0.1974 0.1400 -1.4101 0.1585 -0.4718 0.0770 ## ## intrcpt ## sex ## donorcodeTypical ## sex:donorcodeTypical ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 "],
["pitfalls.html", "9.5 Common pitfalls", " 9.5 Common pitfalls As we have mentioned before, multiple meta-regression, while very useful when applied properly, comes with certain caveats we have to know and consider when fitting a model. Indeed, some argue that (multiple) meta-regression is often improperly used and interpreted in practice, leading to a low validity of many meta-regression models (Higgins and Thompson 2004). Thus, there are some points we have to keep in mind when fitting multiple meta-regression models, which we will describe in the following. Overfitting: seeing a signal when there is none To better understand the risks of (multiple) meta-regression models, we have to understand the concept of overfitting. Overfitting occurs when we build a statistical model which fits the data too closely. In essence, this means that we build a statistical model which can predict the data at hand very well, but performs bad at predicting future data it has never seen before. This happens if our model assumes that some variation in our data stems from a true “signal” in our data, when in fact we only model random noise (Iniesta, Stahl, and McGuffin 2016). As a result, our statistical model produces false positive results: it sees relationships where there are none. Illustration of an overfitted model vs. model with a good fit Regression methods, which usually utilize minimization or maximization procedures such as Ordinary Least Squares or Maximum Likelihood estimatation, can be prone to overfitting (Gigerenzer 2004, 2008). Unfortunately, the risk of building a non-robust model, which produces false-positive results, is even higher once we go from conventional regression to meta-regression (Higgins and Thompson 2004). There are several reasons for this: In Meta-Regression, our sample of data is mostly small, as we can only use the synthesized data of all analyzed studies \\(k\\). As our meta-analysis aims to be a comprehensive overview of all evidence, we have no additional data on which we can “test” how well our regression model can predict high or low effect sizes. In meta-regressions, we have to deal with the potential presence of effect size heterogeneity (see Chapter 6). Imagine a case in which we have two studies with different effect sizes and non-overlapping confidence intervals. Every variable which has different values for the different studies might be a potential explanation for effect size difference we find, while it seems straightforward that most of such explanations are spurious (Higgins and Thompson 2004). Meta-regression as such, and multiple meta-regression in particular, make it very easy to “play around” with predictors. We can test numerous meta-regression models, include many more predictors or remove them in an attempt to explain the heterogeneity in our data. Such an approach is of course tempting, and often found in practice, because we, as meta-analysts, want to find a significant model which explains why effect sizes differ (J. Higgins et al. 2002). However, such behavior massively increases the risk of spurious findings (Higgins and Thompson 2004), because we can change parts of our model indefinitely until we find a significant model, which is then very likely to be overfitted (i.e., it mostly models statistical noise). Some guidelines have been proposed to avoid an excessive false positive rate when building meta-regression models: Minimize the number of investigated predictors a priori. Predictor selection should be based on predefined scientific or theoretical questions we want to answer in our meta-regression. When evaluating the fit of a meta-regression model, we prefer models which achieve a good fit with less predictors. Use fit indices that balance fit and parsimony, such as the Akaike and Bayesian information criterion, to determine which model to retain if you compare several models. When the number of studies is low (which is very likely to be the case), and we want to compute the significance of a predictor, you can use the Knapp-Hartung adjustment to obtain more reliable estimates (J. Higgins et al. 2002), by specifying test = &quot;knha when calling rma(). References "],
["publication-bias.html", "Chapter 10 Publication Bias", " Chapter 10 Publication Bias The file-drawer or publication bias problem is based on the fact that studies with big effect sizes are more likely to be published than studies with small effect sizes (Rothstein, Sutton, and Borenstein 2006). The studies with low effect sizes might never get published, and therefore cannot be integrated into our meta-analysis. This leads to publication bias, as the pooled effect we estimate in our meta-analysis might be higher than the true effect size because we did not consider the missing studies with lower effects due to the simple fact that they were never published. Although this practice is gradually changing (Nelson, Simmons, and Simonsohn 2018), whether a study is published still heavily depends on the statistical significance (\\(p&lt;0.05\\)) of its results (Dickersin 2005). For any sample size, a result is more likely to become statistically significant if its effect size is high. This is particularly true for small studies, for which very large effect sizes are needed to attain a statisitcally significant result. References "],
["smallstudyeffects.html", "10.1 Detecting publication bias", " 10.1 Detecting publication bias Various methods to assess and control for publication bias have been developed, but we will only focus on a few well-established ones here. According to Borenstein et al. (Borenstein et al. 2011). The model behind the most common small-study effects methods has these core assumptions: Because they involve large commitment of ressources and time, large studies are likely to get published, weather the results are significant or not Moderately sized studies are at greater risk of missing, but with a moderate sample size even moderately sized effects are likely to become significant, which means that only some studies will be missing Small studies are at greatest risk for being non-significant, and thus being missing. Only small studies with a very large effect size become significant, and will be found in the published literature. In accordance with these assumptions, the methods we present here particularly focus on small studies with small effect sizes, and wheather they are missing. 10.1.1 Funnel plots The best way to visualize whether small studies with small effect sizes are missing is through funnel plots. We can generate a funnel plot for our m_re meta-analysis output using the funnel() function in metafor: funnel(m_re, xlab = &quot;Hedges&#39; g&quot;) The funnel plot basically consists of a funnel and two axes: the y-axis showing the Standard Error \\(SE\\) of each study, with larger studies (which have a smaller \\(SE\\)) plotted on top of the y-axis; and the x-axis showing the effect size of each study. Given our assumptions, and in the case when there is no publication bias, all studies would lie symmetrically around our pooled effect size (the vertical line in the middle), within the form of the funnel. When publication bias is present, we would assume that the funnel would look asymmetrical, because only the small studies with a large effect size very published, while small studies without a significant, large effect would be missing. We see from the plot that in the case of our meta-anlysis, there is no obvious publication bias among the smaller studies, but there are a few moderately-sized studies that fall outside of the funnel shape. These are the outliers we investigated earlier. We can also display the name of each study using the studlab parameter. 10.1.2 Contour-enhanced funnel plots An even better way to inspect the funnel plot is through contour-enhanced funnel plots, which help to distinguish publication bias from other forms of asymmetry (Peters et al. 2008). Contour-enhanced funnels include colors signifying the significance level into which the effects size of each study falls (p &lt; .05, p &lt; .01, and p &lt; .001). We can plot such funnels using this code: funnel(m_re, level=c(90, 95, 99), shade=c(&quot;white&quot;, &quot;gray55&quot;, &quot;gray75&quot;), refline=0) We can see in the plot that while some studies have statistically significant effect sizes (the gray areas), others do not (white background). We see that the significant studies are primarily on the right side of the line. 10.1.3 Testing for funnel plot asymmetry using Egger’s test Egger’s test for funnel plot asymmetry (Egger et al. 1997) quantifies the funnel plot asymmetry and performs a statistical test: regtest(m_reg) ## ## Regression Test for Funnel Plot Asymmetry ## ## model: mixed-effects meta-regression model ## predictor: standard error ## ## test for funnel plot asymmetry: z = 1.0337, p = 0.3013 The function uses regression to test the relationship between the observed effect sizes and the standard error of the effect sizes. If this relationship is significant, that might indicate publication bias. However, asymmetry could have been caused by other reasons than publication bias. In this case, there is no significant asymmetry. 10.1.4 Duval &amp; Tweedie’s trim-and-fill procedure Duval &amp; Tweedie’s trim-and-fill procedure (Duval and Tweedie 2000) is also based the funnel plot and its symmetry/asymmetry. When Egger’s test is significant, we can use this method to estimate what the actual effect size would be had the “missing” small studies been published. The procedure imputes missing studies into the funnel plot until symmetry is reached again. The trim-and-fill procedure includes the following five steps (Schwarzer, Carpenter, and Rücker 2015): Estimating the number of studies in the outlying part of the funnel plot Removing (trimming) these effect sizes and pooling the results with the remaining effect sizes This pooled effect is then taken as the center of all effect sizes For each trimmed/removed study, a additional study is imputed, mirroring the effect of the study on the left side of the funnel plot Pooling the results with the imputed studies and the trimmed studies included The trim-and-fill-procedure can be performed using the trimfill function in metafor. For this example, we will first introduce some intentional publication bias: df_bias &lt;- df # Identify the 20 effect sizes with the smallest effect sizes small_effects &lt;- order(df$d)[1:20] # Of these small effect sizes, find the ones with the biggest sampling variance big_variance &lt;- order(df$vi[small_effects], decreasing = TRUE) # Delete these studies: delete_these &lt;- small_effects[big_variance[1:10]] df_bias &lt;- df_bias[-delete_these, ] Now, we re-do some of the analyses: # Fit random-effects model m_bias &lt;- rma(d, vi, data = df_bias) # Test for publication bias is now significant: regtest(m_bias) ## ## Regression Test for Funnel Plot Asymmetry ## ## model: mixed-effects meta-regression model ## predictor: standard error ## ## test for funnel plot asymmetry: z = 2.8682, p = 0.0041 # Carry out trim-and-fill analysis m_taf &lt;- trimfill(m_bias) m_taf ## ## Estimated number of missing studies on the left side: 16 (SE = 4.2698) ## ## Random-Effects Model (k = 62; tau^2 estimator: REML) ## ## tau^2 (estimated amount of total heterogeneity): 0.1127 (SE = 0.0274) ## tau (square root of estimated tau^2 value): 0.3356 ## I^2 (total heterogeneity / total variability): 80.45% ## H^2 (total variability / sampling variability): 5.11 ## ## Test for Heterogeneity: ## Q(df = 61) = 269.1227, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## 0.1480 0.0498 2.9732 0.0029 0.0504 0.2455 ** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 We see that the procedure identified and trimmed 16 studies (with 16 added studies)). The overall effect estimated by the procedure is \\(g = 0.1480\\). Let’s compare this to our initial results. m_re$b[1] ## [1] 0.2392596 The initial pooled effect size was \\(g = 0.2393\\), which is substantially larger than the bias-corrected effect. In our case, if we assume that publication bias was a problem in the analyses, the trim-and-fill procedure lets us assume that our initial results were overestimated due to publication bias, and the “true” effect when controlling for selective publication might be \\(g = 0.1480\\) rather than \\(g = 0.2394\\). Of course, we intentionally introduced the bias in this case. We can also create funnel plots including the imputed studies: # Draw funnel plot with missing studies filled in funnel(m_taf) References "],
["multilevel-meta-analysis.html", "Chapter 11 “Multilevel” Meta-Analysis", " Chapter 11 “Multilevel” Meta-Analysis \\[ \\] By the time you reach this chapter, you have already fitted a multilevel meta-analytic model several times. This is because the meta-analytic random-effects model is, by definition, a two-level multilevel model. When people talk about “multilevel meta-analysis”, however, they more commonly what they often think of are three-level meta-analytic models. We introduce these here, because they are an excellent and convenient solution to the problem of dependent data: The situation that arises when you extract several effect sizes from ONE sample. For example, if a study applies one manipulation, and measures three similar dependent variables, then you can calculate three effect sizes from that study, which will be dependent. The best way to account for this dependency is by taking the sampling covariance (similar to the sampling variance of each effect size) between the effect sizes into account. This information is almost never reported by the original authors, however, so a very attractive solution is the three-level multilevel approach (Van den Noortgate et al. 2015), which merely assumes that the sampling covariances are the same between all pairs of effect sizes in all studies. References "],
["meta-analysis-is-multi-level-optional.html", "11.1 Meta-Analysis is multi-level (optional)", " 11.1 Meta-Analysis is multi-level (optional) To see why meta-analysis is by nature multileveled, let us go back to the formula for the random-effects model we discussed in Chapter 5.2: \\[\\hat\\theta_k = \\mu + \\epsilon_k + \\zeta_k\\] We discussed that the terms \\(\\epsilon_k\\) and \\(\\zeta_k\\) are introduced in a random-effects model because we assume that there are two sources of variability. The first one is caused by the sampling error (\\(\\epsilon_k\\)) of individual studies, which causes their effect size estimates to deviate from the true effect size \\(\\theta_k\\). The second one, \\(\\zeta_k\\) is the between-study heterogeneity caused by the fact that the true effect size of a study \\(k\\) itself is again only part of an overarching distribution of true effect sizes, from which the individual true effect size \\(\\theta_k\\) was drawn. Our aim in the random-effects model is therefore to estimate the mean of this distribution of true effect sizes, \\(\\mu\\). The two error terms correspond with the two levels within our meta-analysis data: the “participant” level (level 1) and the “study” level (level 2). The figure below symbolizes this structure. In the lower level (level 1), we have the participants (and/or patients). These participants are, of course, part of a number of larger units, the studies (level 2), from which we retrieved the data for our meta-analysis. While the data on level 1 usually already reaches us, the meta-analysts, in a “pooled” form (e.g., the authors of the paper provide us with the mean and standard deviation of their studied sample instead of the raw data), pooling on level 2, the study level, has to be performed in meta-analysis. In the tradition of multilevel modeling, such data is called nested data: in most meta-analyses, one can say that participants are “nested” within studies. A simplified illustration of the multilevel structure of conventional meta-analytic models Thus, if we split the formula from above into the components corresponding to the two levels, we get the following formulae: Level 1 (participants) model: \\[\\begin{equation} \\label{eq:1} \\hat\\theta_k = \\theta_k + \\epsilon_k \\end{equation}\\] Level 2 (studies) model: \\[\\begin{equation} \\label{eq:2} \\theta_k = \\mu + \\zeta_k \\end{equation}\\] You might have already detected that we can substitute \\(\\theta_k\\) in the first equation with its definition in the second equation. What we then get is exactly the generic formula for the meta-analytic model from before (even a fixed-effects model can be defined as having a multilevel structure, yet with \\(\\zeta_k\\) being zero). Thus, it becomes evident that the way we define a meta-analytic model already has multilevel properties “built-in” given that we assume that participants are nested within studies in our data. 11.1.1 Three-level meta-analytic models It has become clear that meta-analysis naturally possesses a multilevel structure. This allows us to expand this structure even further to better reflect our data. Three-level models (Cheung 2014; Assink and Wibbelink 2016) are a good way to do this. Statistical independence is one of the core assumptions of meta-analytic pooling (Hedges 2009). If there is a dependency between effect sizes (i.e., the effect sizes are correlated), this may artificially reduce heterogeneity and thus lead to false-positive results. Dependence may stem from different sources (Cheung 2014): Dependence introduced by the authors of the individual studies. For example, scientists conducting the study might have collected data from multiple sites, or they might have compared multiple interventions to one single control group, or they might have used different questionnaires to measure an outcome of interest (e.g., they used the BDI as well as the PHQ-9 to assess depression in patients). For all of these scenarios, we can assume that some kind of dependency is introduced within the reported data. Dependence introduced by the meta-analyst herself. For example, a meta-analysis could focus on a specific psychological mechanism and include studies which were conducted in different cultural regions of the world. It seems reasonable that the reported results are more similar when studies were conducted in the same cultural context. We can take such dependencies into account by integrating a third layer into the structure of our meta-analysis model. For example, one could model that different questionnaires are nested within studies. Or one could create a model in which studies are nested within cultural regions. This creates a three-level meta-analytic model, as illustrated by the figure below. The generic mathematical formulae for a three-level meta-analytic model look like this: Level 1 model \\[\\hat\\theta_{ij} = \\theta_{ij} + \\epsilon_{ij}\\] Level 2 model \\[\\theta_{ij} = \\kappa_{j} + \\zeta_{(2)ij}\\] Level 3 model \\[\\kappa_{j} = \\beta_{0} + \\zeta_{(3)j}\\] Where \\(\\theta_{ij}\\) is the true effect size, \\(\\hat\\theta_{ij}\\) its estimator in the \\(i\\)th effect size in cluster \\(j\\), \\(\\kappa_{j}\\) the average effect size in \\(j\\) and \\(\\beta_0\\) the average population effect. Again, we can piece these formulae together and get this: \\[\\hat\\theta_{ij} = \\beta_{0} + \\zeta_{(2)ij} + \\zeta_{(3)j} + \\epsilon_{ij}\\] References "],
["fitting-a-three-level-model.html", "11.2 Fitting a three-level model", " 11.2 Fitting a three-level model The metafor package is particularly well suited for fitting various three-level models in meta-analyses. 11.2.1 Data preparation For this chapter, we continue to use the curry dataset from metaforest, which we have assigned to the object df. It has already been prepared for multilevel model fitting: Let’s have a peek at the dataset. # Load the package, if you haven&#39;t yet library(metaforest) # Assign the curry dataset to df, if you haven&#39;t yet df &lt;- curry # Examine the first 6 rows of the data head(df) We see that the first two columns of the dataset are ID variables: study_id and effect_id. In these two columns, we stored the identifiers corresponding to the individual effect sizes, and we will use the studies as clusters on different levels of our multilevel model. Multilevel models can accommodate different sources of heterogeneity, too. Maybe you have effect sizes originating in different countries, or maybe you want to cluster by (team of) authors. Just replace the study_id with your cluster id, in those cases. It is even possible to have more levels added (e.g., effect sizes within studies within countries), although that is beyond the scope of this tutorial. 11.2.2 Model fitting We are now prepared to fit our model. I will save the model to the object m_multi. For model fitting, we use the rma.mv function in metafor. This is a more technical interface than the standard rma function. The function has plenty of parameters one can specify (type ?metafor::rma.mv in the console to see them all). For now, we will only focus on the really essential ones. Code Description yi The variable in our dataset containing the effect size data V The variable in our dataset containing the sampling variance data corresponding to each y random The model specifications for our multilevel model. We describe this in more detail below data The data.frame containing our data method The tau-squared estimator. Our options here are limited, and it is advised to use the restricted maximum likelihood (‘REML’) method, which is the default Setting up the formula Under the random parameter, we feed rma.mv with the formula for the random effects in our model. As we actually have two formulas in three-level models, we have to bundle them together in a list(). The notation for rma.mv is very similar to other packages specialized for mixed-effects models such as lme4 (Bates et al. 2015). Within the list(), formulae are separated with commas. Within the formula, we first define the random effects variance as ~ 1, followed by a vertical bar |. Behind the vertical bar, we assign this random effect to a grouping variable such as studies, measures, regions and so forth. We only have to define the formulae for level 2 and level 3. The sampling variance in level 1 is assumed to be known; this is a necessary assumption in three-level meta-analysis, because we don’t have the raw data. Without this assumption, the model would not be identified. We type in the formulae in the order of our multilevel model. As in our example, ID_2 is nested in ID_1, we first define level 2 as ~ 1 | ID_2 and then level 3 as ~ 1 | ID_1. Now, let’s put it all together: m_multi &lt;- rma.mv(d, vi, random = list(~ 1 | effect_id, ~ 1 | study_id), data = df) m_multi ## ## Multivariate Meta-Analysis Model (k = 56; method: REML) ## ## Variance Components: ## ## estim sqrt nlvls fixed factor ## sigma^2.1 0.0000 0.0000 56 no effect_id ## sigma^2.2 0.0838 0.2895 27 no study_id ## ## Test for Heterogeneity: ## Q(df = 55) = 156.9109, p-val &lt; .0001 ## ## Model Results: ## ## estimate se zval pval ci.lb ci.ub ## 0.2836 0.0650 4.3638 &lt;.0001 0.1562 0.4109 *** ## ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Let’s go through the output one by one. First, let’s have a look at the Variance Components. Here, we see the variance calculated for each level of our model, \\(\\sigma^2_1\\) (level 2) and \\(\\sigma^2_2\\) (level 3). Under nlvls, we see the number of levels (i.e., subgroups) on each level. The number 56 corresponds to the number of unique effect sizes, and 27 is the number of unique studies. The heading factor is the best clue as to what levels these are: effect_id is the within-studies variance componend, and study_id is the between-studies variance component. Under Model Results, we see the estimate of our pooled effect, which is 0.2836. As our y column contained effect sizes expressed as Hedges’ g, the effect is therefore \\(g = 0.2836.\\), with a confidence interval of \\(0.16-0.41\\). Under Test for Heterogeneity, we see that the variance of effect sizes within our data overall was significant (\\(p &lt; 0.001\\)). This however, is not very informative as we are interested how variance can be attributed to the different levels in our model. 11.2.3 Distribution of total variance As mentioned before, what we’re actually interested in is the distribution of variance over the three levels of our model. Cheung (2014) provides a formula to estimate the sampling variance, which is needed to calculate the variance proportions for each level. We have have programmed a function called variance_decomposition, which implements this formula. The code for the function can be seen below. Parts of this code were adapted from Assink and Wibbelink (Assink and Wibbelink 2016). R doesn’t know this function yet, so we have to let R learn it copying and pasting the code underneath in its entirety into the console on the bottom left pane of RStudio, and then hit Enter ⏎. variance_decomposition &lt;- function(m){ n &lt;- m$k vector.inv.var &lt;- 1/(diag(m$V)) sum.inv.var &lt;- sum(vector.inv.var) sum.sq.inv.var &lt;- (sum.inv.var)^2 vector.inv.var.sq &lt;- 1/(diag(m$V)^2) sum.inv.var.sq &lt;- sum(vector.inv.var.sq) num &lt;- (n-1)*sum.inv.var den &lt;- sum.sq.inv.var - sum.inv.var.sq est.samp.var &lt;- num/den if(length(m$sigma2) &gt; 2) stop(&quot;Cannot handle more than three levels.&quot;) total_var &lt;- (sum(m$sigma2)+est.samp.var)/100 Variance &lt;- c(est.samp.var, m$sigma2)/total_var names(Variance) &lt;- c(&quot;Level1&quot;, m$s.names) Variance } Let’s have a look: variance_decomposition(m_multi) ## Level1 effect_id study_id ## 2.445403e+01 1.443707e-08 7.554597e+01 From the outputs, we see that about 24.45% of the overall variance can be attributed to level 1, 0% to level 2 (within-study variance), and as much as 75.55% to level 3 (between-study variance). 11.2.4 Comparing the fit Of course, when fitting a three-level model, we are interested if this model actually captures the variability in our data better than a two-level model. metafor allows us to easily do this by comparing models in which one of the levels is removed. To do this, we can use the rma.mv function again, but this time, we hold the variance component \\(\\sigma^2\\) of one level constant at 0. This can be done by specifying the sigma2 parameter. The parameter can be specified by providing a vector telling the function which level to hold constant, with the generic version being c(level 2, level3). So, if we want to hold level 2 constant while leaving the rest as is, we use sigma2 = c(0,NA), and vice versa for the third level. We can then use ANOVAs to compare the fit of these models. m_within_null &lt;- rma.mv(yi = d, V = vi, random = list(~ 1 | effect_id, ~ 1 | study_id), sigma2 = c(0, NA), data = df) m_between_null &lt;- rma.mv(yi = d, V = vi, random = list(~ 1 | effect_id, ~ 1 | study_id), sigma2 = c(NA, 0), data = df) m_both_null &lt;- rma.mv(yi = d, V = vi, random = list(~ 1 | effect_id, ~ 1 | study_id), sigma2 = c(0, 0), data = df) aov_within &lt;- anova(m_multi, m_within_null) aov_between &lt;- anova(m_multi, m_between_null) aov_bothnull &lt;- anova(m_multi, m_both_null) # Join these results in a table aov_table &lt;- rbind( c(df=aov_between$p.f, aov_between$fit.stats.f[c(3:4, 1)], LRT = NA, p = NA), c(df=aov_within$p.r, aov_within$fit.stats.r[c(3:4, 1)], LRT = aov_within$LRT, p = aov_within$pval), c(df=aov_between$p.r, aov_between$fit.stats.r[c(3:4, 1)], LRT = aov_between$LRT, p = aov_between$pval), c(df=aov_bothnull$p.r, aov_bothnull$fit.stats.r[c(3:4, 1)], LRT = aov_bothnull$LRT, p = aov_bothnull$pval) ) rownames(aov_table) &lt;- c(&quot;Three-level model&quot;, &quot;Within-studies variance constrained&quot;, &quot;Between-studies variance constrained&quot;, &quot;Both variance components constrained&quot;) df AIC BIC ll LRT p Three-level model 3 22.14 28.16 -8.07 Within-studies variance constrained 2 20.14 24.15 -8.07 0.00 1.000 Between-studies variance constrained 2 32.74 36.76 -14.37 12.61 0.000 Both variance components constrained 1 78.24 80.25 -38.12 60.10 0.000 This table allows us to easily compare the models, using fit indices (AIC and BIC; lower is better), and likelihood ratio tests and p-values. The results unanimously indicate that the model with within-studies variance constrained to 0 fits the best, and that this constraint does not lead to significantly worse fit (p = 1.00). Still, the decision which model to use should be based at least as much on theory, as on statistical criteria. References "],
["metaforest.html", "Chapter 12 Exploring heterogeneity", " Chapter 12 Exploring heterogeneity This chapter is heavily based on my in-press, open access book chapter “Small sample meta-analyses: Exploring heterogeneity using MetaForest”. The book does not yet have a DOI, but information about it will be anounced here: https://s4.wp.hum.uu.nl/. A valid reference for the methods described in this chapter is Van Lissa (2017). In the social sciences, meta-analyses often pool research conducted in different laboratories, using different methods, instruments, and samples. Such between-studies differences can introduce substantial heterogeneity in the effect sizes found. At the same time, the sample of studies is often small, which means there is limited statistical power to adequately account for moderators that cause between-studies heterogeneity. If we just include all moderators in a meta-regression, we risk overfitting the data. In this chapter, I introduce a technique that can explore between-studies heterogeneity and perform variable selection, identifying relevant moderators from a larger set of candidates, without succumbing to overfitting: MetaForest, a machine-learning based approach to identify relevant moderators in meta-analysis (Van Lissa 2017). References "],
["understanding-random-forests.html", "12.1 Understanding random forests", " 12.1 Understanding random forests MetaForest is an adaptation of the random forests algorithm (see Strobl, Malley, and Tutz 2009) for meta-analysis. Random forests are a powerful machine learning technique, with several advantages. Firstly, random forests are robust to overfitting. Secondly, they are a non-parametric technique, which means that they can easily capture non-linear relationships between the moderator and effect size, or even complex, higher-order interactions between moderators. Thirdly, random forests perform variable selection, identifying which moderators contribute most strongly to the effect size found. The random forest algorithm combines many tree models. A tree model can be conceptualized as a decision tree, or a flowchart: The model recursively splits the data into groups with maximally similar values on the outcome variable, the study effect size. The splitting decisions are based on the moderator variables. Starting with the full dataset, the model first finds the moderator variable, and the value on that variable, along which to split the dataset. It chooses the moderator and value that result in the most homogenous post-split groups possible. This process is repeated for each post-split group; over and over again, until a stopping criterion is reached. Usually, the algorithm is stopped when the post-split groups contain a minimum number of cases. One advantage of regression trees is that it does not matter if the number of moderators is large relative to the sample size, or even exceeds it. Secondly, trees are non parametric; they do not assume normally distributed residuals or linearity, and intrinsically capture non-linear effects and interactions. These are substantial advantages when performing meta-analysis on a heterogeneous body of literature. Single regression trees also have a limitation, however, which is that they are extremely prone to overfitting. They will simply capture all patterns in the data, both genuine effects and random noise (Hastie, Tibshirani, and Friedman 2009). Random forests overcome this limitation of single regression trees. First, many different bootstrap samples are drawn (say 1000). Then, a single tree is grown on each bootstrap sample. To ensure that each tree learns something unique from the data, only a small random selection of moderators is made available to choose from at each splitting point. Finally, the predictions of all tree models are averaged. This renders random forests robust to overfitting: Because each tree captures some of the true patterns in the data, and overfits some random noise that is only present in its bootstrap sample, overfitting cancels out on aggregate. Random forests also make better predictions: Where single trees predict a fixed value for each “group” they identify in the data, random forests average the predictions of many trees, which leads to smoother prediction curves. 12.1.1 Meta-analytic random forests To render random forests suitable for meta-analysis, a weighting scheme is applied to the bootstrap sampling, which means that more precise studies exert greater influence in the model building stage (Van Lissa 2017). These weights can be uniform (each study has equal probability of being selected into the bootstrap sample), fixed-effects (studies with smaller sampling variance have a larger probability of being selected), or random-effects based (studies with smaller sampling variance have a larger probability of being selected, but this advantage is diminished as the amount of between-studies heterogeneity increases). Internally, metaforest relies on the ranger R-package; a fast implementation of the random forests in C++. 12.1.2 Tuning parameters Like many machine learning algorithms, random forests have several “tuning parameters”: Settings that might influence the results of the analysis, and whose optimal values must be determined empirically. The first is the number of candidate variables considered at each split of each tree. The second is the minimum number of cases that must remain in a post-split group within each tree. The third is unique to MetaForest; namely, the type of weights (uniform, fixed-, or random-effects). The optimal values for these tuning parameters are commonly determined using cross-validation (Hastie, Tibshirani, and Friedman 2009). Cross-validation means splitting the dataset many times, for example, into 10 equal parts. Then, predictions are made for each of the parts of the data, using a model estimated on all of the other parts. This process is conducted for all possible combinations of tuning parameters. The values of tuning parameters that result in the lowest cross-validated prediction error are used for the final model. For cross-validation, metaforest relies on the well-known machine learning R-package caret. References "],
["using-metaforest.html", "12.2 Using MetaForest", " 12.2 Using MetaForest To illustrate how to use MetaForest to identify relevant moderators in a small sample meta-analysis, I will apply it to the curry data. # Load the metaforest package library(metaforest) # Select only the relevant variables from the curry data data &lt;- curry[, c(&quot;d&quot;, &quot;vi&quot;, &quot;study_id&quot;, &quot;sex&quot;, &quot;age&quot;, &quot;location&quot;, &quot;donorcode&quot;, &quot;interventioncode&quot;, &quot;controlcode&quot;, &quot;outcomecode&quot;)] 12.2.1 Checking convergence For any random forests model, it is important to check whether the model converges. Convergence is assessed by examining the cumulative mean squared out-of-bag prediction error (MSE), as a function of the number of trees in the model. When the MSE stabilizes, the model is said to have converged. To get an impression of how many trees are required to have the model converge, we will run the analysis once with a very high number. We then pick a smaller number of trees, at which the model is also seen to have definitely converged, to speed up the subsequent computationally heavy steps, such as replication and model tuning. We will examine convergence again for the final model. # Because MetaForest uses the random number generator (for bootstrapping), # we set a random seed so analyses can be replicated exactly. set.seed(242) # Run model with many trees to check convergence check_conv &lt;- MetaForest(d~., data = data, study = &quot;study_id&quot;, whichweights = &quot;random&quot;, num.trees = 20000) # Plot convergence trajectory plot(check_conv) It can be seen that this model has converged within approximately 5000 trees. Thus, we will use this number of trees for subsequent analyses. We now apply recursive pre-selection using the preselect function. This algorithm helps eliminate noise moderators by running the analysis, dropping the moderator with the most negative variable importance, and then re-running the analysis until all remaining variables have positive importance. This recursive algorithm is replicated 100-fold. Using preselect_vars, we retain only those moderators for which a 50% percentile interval of the variable importance metrics does not include zero (variable importance is counted as zero when a moderator is not included in the final step of the recursive algorithm). set.seed(55) # Model with 10000 trees for replication mf_rep &lt;- MetaForest(d~., data = data, study = &quot;study_id&quot;, whichweights = &quot;random&quot;, num.trees = 5000) # Run recursive preselection, store results in object &#39;preselect&#39; preselected &lt;- preselect(mf_rep, replications = 100, algorithm = &quot;recursive&quot;) # Plot the results plot(preselected) # Retain only moderators with positive variable importance in more than # 50% of replications retain_mods &lt;- preselect_vars(preselected, cutoff = .5) We can see that only interventioncode and location have been selected. "],
["model-tuning.html", "12.3 Model tuning", " 12.3 Model tuning Next, we tune the model using the R-package caret, which offers a uniform workflow for any machine learning task. The function ModelInfo_mf tells caret how to tune a MetaForest analysis. As tuning parameters, we consider all three types of weights (uniform, fixed-, and random-effects), number of candidate variables at each split from 2-6, and a minimum node size from 2-6. We select the model with smallest prediction error (RMSE) as final model, based on 10-fold clustered cross-validation. Clustered cross-validation means that effect sizes from the same study are always included in the same fold, to account for the dependency in the data. Note that the number of folds cannot exceed the number of clusters in the data. Moreover, if the number of clusters is very small, one might have to resort to specifying the same number of folds as clusters. Model tuning typically takes a long time; for this small dataset, it might take five minutes; for a very large dataset, it might take hours. However, when your dataset is larger, you will often reach model convergence at a lower number of trees, which reduces computation time. # Install the install.packages(&quot;caret&quot;) # Load the caret library library(caret) # Set up 10-fold grouped (=clustered) CV grouped_cv &lt;- trainControl(method = &quot;cv&quot;, index = groupKFold(data$study_id, k = 10)) # Set up a tuning grid for the three tuning parameters of MetaForest tuning_grid &lt;- expand.grid(whichweights = c(&quot;random&quot;, &quot;fixed&quot;, &quot;unif&quot;), mtry = 1:2, min.node.size = 1:10) # X should contain only retained moderators, clustering variable, and vi X &lt;- data[, c(&quot;study_id&quot;, &quot;vi&quot;, retain_mods)] set.seed(78) # Train the model mf_cv &lt;- train(y = data$d, x = X, study = &quot;study_id&quot;, # Name of the clustering variable method = ModelInfo_mf(), trControl = grouped_cv, tuneGrid = tuning_grid, num.trees = 5000) # Extract R^2_{cv} for the optimal tuning parameters r2_cv &lt;- mf_cv$results$Rsquared[which.min(mf_cv$results$RMSE)] Based on the root mean squared error, the best combination of tuning parameters were fixed-effect weights, with 1 candidate variable per split, and a minimum of 9 cases per terminal node. The object returned by train already contains the final model, estimated with the best combination of tuning parameters. Consequently, we can proceed directly to reporting the results. First, we examine convergence again. Then, we examine the \\(R^2_{oob}\\). # For convenience, extract final model final &lt;- mf_cv$finalModel # Extract R^2_{oob} from the final model r2_oob &lt;- final$forest$r.squared # Plot convergence plot(final) The final model has clearly converged. We can check the two estimates of variance explained in new data: \\(R^2_{cv}\\), which is based on the 10-fold grouped cross-validation, and \\(R^2_{oob}\\), which is based on cases outside of the bootstrap samples of the individual trees. r2_cv ## [1] 0.4985625 r2_oob ## [1] -0.1625724 We can see that the \\(R^2_{cv} = .50\\), and the \\(R^2_{oob} = -.16\\). The negative value means that the model performs worse than just using the mean. The fact that these values differ so much suggests that perhaps our model is not detecting reliable patterns in the data. This would also correspond with the fact that none of the variables were consistently selected during the recursive preselection step. In the published paper for this meta-analysis, we bootstrapped the entire analysis 100 times, and showed that the average bootstrapped \\(R^2\\) was around 0. So we concluded that none of the moderators were relevant. For the sake of this exercise, we still continue exploring, however! 12.3.1 Variable importance variable importance metrics, which quantify the relative importance of each moderator in predicting the effect size. These metrics are analogous in function to the (absolute) standardized regression coefficients (beta; \\(\\mid\\beta\\mid\\)) in regression: They reflect the strength of each moderator’s relationship with the outcome on a common metric. However, whereas betas in regression only reflect linear, univariate, partial relationships, MetaForest’s variable importance metrics reflect each moderator’s contribution to the predictive power of the final model across all linear-, non-linear-, and interaction effects. So-called permutation importance is obtained by randomly permuting, or shuffling, the values of a moderator, thereby anulling any relationship that moderator had with the outcome, and then observing how much the predictive performance of the final model drops. If predictive performance drops a lot, the moderator must have been important. Plot the variable importance: # Plot variable importance VarImpPlot(final) 12.3.2 Partial dependence Partial dependence plots visualize the shape of the marginal relationship of each moderator to the effect size, averaging over all values of the other predictors. Researchers most commonly inspect only univariate marginal dependence plots. Exploring all possible higher order interactions swiftly becomes unmanageable; with just 10 moderators, the number of bivariate interactions is 45, and the number of trivariate interactions is 120. In order to plot bivariate interactions with a specific moderator of theoretical relevance, you can use the PartialDependence function in conjunction with the moderator argument. Because this is an exploratory, non-parametric analysis, we cannot conclude whether any of these findings are “significant”. However, the PartialDependence function has two settings that help visualize the “importance” of a finding: rawdata, which plots the weighted raw data (studies with larger weights are plotted with a larger point size), thereby visualizing the variance around the mean prediction, and pi, which plots a (e.g., 95%) percentile interval of the predictions of individual trees in the model. This is not the same as a confidence interval, but it does show how variable or stable the model predictions are. PartialDependence(final, rawdata = TRUE, pi = .95) The partial dependence plots show that acts of kindness had a somewhat smaller effect than other interventions, and that studies conducted in the USA had slightly higher effects than others. But there are some factor levels with very few cases, and there is a lot of overlap between the distributions across levels. Thus, it is not surprising that we found no significant effect of location in the meta-regression model of earlier exercises. Finally, if you want to explore bivariate interactions, you can specify one moderator variable. With this dataset, this plot does not make much sense, because there is so much missing data in combinations of factors. But the syntax for a moderated partial dependence plot is: PartialDependence(final, moderator = &quot;interventioncode&quot;, rawdata = TRUE, pi = .95) "],
["what-to-report.html", "12.4 What to report", " 12.4 What to report The preceding paragraphs offer a step-by-step instruction on how one might go about conducting a MetaForest analysis on a small sample meta-analytic dataset. One could simply apply these steps to a different dataset. However, reporting each step in detail might raise more questions than it answers, especially with readers and Reviewers unfamiliar with the machine learning approach in general, and MetaForest in specific. At the same time, it is essential that the analysis process is reproducible and transparent. This can be achieved by publishing the entire annotated syntax of the analysis as supplementary material, for example, on the Open Science Framework (www.osf.io). In fact, best practice would be to go one step further, and share the full data along with the syntax. In the text of the paper, one might then simply report a summary of the analysis, and provide a hyperlink to the DOI of the OSF page. The part of the results section describing the MetaForest analysis might read something like this: “We conducted an exploratory search for relevant moderators using MetaForest: a machine-learning based approach to meta-analysis, using the random forests algorithm (Van Lissa 2017). Full syntax of this analysis is available on the Open Science Framework, DOI:10.17605/OSF.IO/XXXXX. To weed out irrelevant moderators, we used 100-fold replicated feature selection, and retained only moderators with positive variable importance in &gt; 10% of replications. The main analysis consisted of 10.000 regression trees with fixed-effect weights, four candidate variables per split, and a minimum of three cases per terminal node. The final model had positive estimates of explained variance in new data, \\(R^2_{oob} = \\dots\\), \\(R^2_{cv} = \\dots\\). The relative importance of included moderators is displayed in Figure X. The shape of each moderator’s marginal relationship to the effect size, averaging over all values of all other moderators, is illustrated in Figure XX.” References "],
["references.html", "References", " References "]
]
